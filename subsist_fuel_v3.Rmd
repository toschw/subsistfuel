---
title: "Subsistence fuel analysis"
author: "Tobias Schwoerer"
date: "June 5 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Data import and exploration
What probability distribution best fits the data?
```{r}
library(dplyr)
library(tidyr)
library(MASS)
library(tidybayes)
library(rstanarm)
library(ggplot2)
library(car)


#data <- read.csv("C:/Users/Toby/Dropbox/DATA/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/Subset4R.csv",stringsAsFactor=FALSE)
data <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/Subset4R.csv",stringsAsFactor=FALSE)

#eliminating missing data, equipment not used, chainsaws, and generators
data2 <- subset(data, !is.na(an_gal))
data2 <- subset(data2, used=="Used this equipment")

data2 <- data2%>%
  filter(resname!="Chainsaw" & resname!="Generator")

#grouping data: each row = one distinct household
hh_data <- data2%>%
  group_by(ID)%>%
  mutate(an_gal_total=sum(an_gal),
         usenum_total=sum(usenum),
         sms = sum(sm),
         atvs = sum(atv),
         cars =sum(car),
         boats = sum(boat))%>%
  subset(!is.na(price), select=-c(an_gal, survey_id, UniqueID, resname, fuel, saw, sm, atv, car, boat, vehicles, usenum))%>%
  distinct(ID, .keep_all = TRUE)

#eliminating records that are below 1gal in gasoline consumption. 
hh_data2 <- hh_data%>%
  subset( an_gal_total>1)

#check whether outcome variable is normally or log-normally distributed
Aplot <- car::qqPlot(hh_data2$an_gal_total, xlab="normal quantiles", ylab="gasoline consumption", envelope=0.95, id=FALSE)
Bplot <- qqPlot(hh_data2$an_gal_total, "lnorm", xlab="log-normal quantiles", ylab="gasoline consumption", envelope=0.95, id=FALSE)


#eliminating extreme outliers based on distribution fit plot, eliminated households using more than 5000 gallons and harvesting more than 8000lbs.
hh_data3 <- hh_data2%>%
  filter(sum_edible_wei_lbs < 8000 & an_gal_total < 5000 )
#& commname!="Dot Lake"
#recheck log-normal distribution
Cplot <- qqp(hh_data3$an_gal_total, "lnorm", xlab="log-normal quantiles", ylab="gasoline consumption", envelope=0.95, id=FALSE)


#rescaling weight data into 1000s of lbs, 
hh_data3 <- hh_data3%>%
  mutate(FishSumEdible1000 = FishSumEdible/1000,
         LargeGameSumEdible1000 = LargeGameSumEdible/1000)
```
#eliminating old price columns, adding new column with updated gasoline prices
Source: 
Department of Commerce Community and Economic Development Division of Community and Regional Affairs Research and Analysis
https://www.commerce.alaska.gov/web/Portals/4/pub/FuelWatch_2011_June.pdf
```{r}
gasoline <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/gasoline_prices.csv",stringsAsFactor=FALSE)
hh_data3 <- hh_data3%>%
  left_join(gasoline, by="commname")
```
Result: 
an_gal_total (annual gasoline consumption in gallons per household) is log-normally distributed, need to transform (https://github.com/stan-dev/rstanarm/issues/115). Data such as the gasoline consumption are often naturally log-normally distributed:  values are often low, but are occasionally high or very high (https://rcompanion.org/handbook/I_12.html)

##Transforming outcome variable
```{r}
#log-transforming an_gal_total
hh_data3$gal_log <- log(hh_data3$an_gal_total)
#Checking normal distribution fit
Dplot <- qqp(hh_data3$gal_log, "norm", xlab="normal quantiles", ylab="log(gasoline consumption)",envelope=0.95, id=FALSE)
```

#Model estimation
##Bayesian Generalized Linear Models With Group-Specific Terms Via Stan
Fitting a lognormal data model using the log-transformed outcome variable, gal_log. 
Variables that ended up close to zero and not adding any predictive power were: OtherSumEdible, SmallGameSumEdible, VegetationSumEdible, and LargeGameSumDist. 
###Preparing data for model
```{r}
#dropping other price columns and eliminating variables we don't need
keep <- c("an_gal_total", "gal_log", "FishSumEdible1000", "LargeGameSumEdible1000", "usenum_total", "atvs", "sms", "boats", "cars", "gasolinePrice", "commname")
modelData <- hh_data3[keep]
modelData <- modelData%>%
  na.omit()


```

###Testing for homogeneity between communities, which exists if the p-value of the test is less than the significance level of 0.05. 
In order to get valid parameter estimates, we need more or less a homogeneous response variable across groups (communities). See: Gabry and Goodrich 2018: https://cran.r-project.org/web/packages/rstanarm/vignettes/glmer.html  "An analysis that disregards between-group heterogeneity can yield parameter estimates that are wrong if there is between-group heterogeneity but would be relatively precise if there actually were no between-group heterogeneity."

Since the log-transformed data is not exactly normally distributed, we use the Levene test as a more robust alternative to test for the above. 

H0: the group variances are equal (called homogeneity of variance or homoscedasticity)
If the resulting p-value of Levene's test is less than some significance level (typically 0.05), the obtained differences in sample variances are unlikely to have occurred based on random sampling from a population with equal variances. Thus, the null hypothesis of equal variances is rejected and it is concluded that there is a difference between the variances in the population. In other words, there is heteroscedasticity. 
```{r}
library(car)
#Bartlett's test 
bartlett.test(gal_log ~ commname, data = modelData)
# Levene's test with one independent variable. The Levene test, is a more robust alternative to the Bartlett test when the distributions of the data are non-normal.
leveneTest(gal_log ~ commname, data = modelData)

fligner.test(gal_log ~ commname, data = modelData)
```
Result:
In both tests, the H0 is rejected, heterogeneous variances exists between communities. Concluding our estimators are biased. 

Bayesian estimation can help in this case:
"Group-by-group analyses, on the other hand, are valid but produces estimates that are relatively imprecise. While complete pooling or no pooling of data across groups is sometimes called for, models that ignore the grouping structures in the data tend to underfit or overfit (Gelman et al.,2013). Hierarchical modeling provides a compromise by allowing parameters to vary by group at lower levels of the hierarchy while estimating common parameters at higher levels. Inference for each group-level parameter is informed not only by the group-specific information contained in the data but also by the data for other groups as well. This is commonly referred to as borrowing strength or shrinkage." (Gabry and Goodrich 2018)

###3 HB models
```{r}
#Model HB1
Mglm <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars +   (1 + gasolinePrice | commname), data = modelData, family = gaussian(link = "identity"))


#Model HB2
Mglm2 <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars +   (1 | commname), data = modelData, family = gaussian(link = "identity"))
launch_shinystan(Mglm2)

#model summary output
prior_summary(object = Mglm2)
Mglm2
summary(Mglm2)

#Evaluating model convergence and diagnose model through STAN online portal
plot(Mglm2, "rhat")
plot(Mglm2, "ess")

#Model HB3, an even simpler model without vehicle stock
Mglm3 <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   +   (1 | commname), data = modelData, family = gaussian(link = "identity"))
```

#Diagnostics for Model2
##Predictive checks
```{r}
library(bayesplot)
library(rstan)
if (!require("devtools")) {
  install.packages("devtools")
}
devtools::install_github("stan-dev/bayesplot")

#predictive checking
theme_set(theme_cowplot(font_size=7))
color_scheme_set("brightblue")
Tmean <- Mglm2 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm2$y,
             stat = "mean")
Tsd <- Mglm2 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm2$y,
             stat = "sd")
Tmin <- Mglm2 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm2$y,
             stat = "min")
Tmax <- Mglm2 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm2$y,
             stat = "max")
#combining plots
library(cowplot)
theme_set(theme_cowplot(font_size=7))
plot_grid(Tmean, Tsd, Tmin, Tmax, labels = c("A", "B", "C", "D"), ncol = 2)
ggsave("yyrepGrid_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 140, height = 140, units = "mm",
  dpi = 300, limitsize = TRUE)
```


#Results
##Model checking
```{r}
#convergence check after warm-up
color_scheme_set("mix-blue-red")
con_test_fig <- mcmc_trace(Mglm, pars = c("FishSumEdible1000", "sigma"),
           facet_args = list(nrow = 2)) +
   ggplot2::theme_update(text = element_text(size=7))  
con_test_fig
ggsave("con_test_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```

##Posterior distributions for estimators 
```{r}
posterior <- as.matrix(Mglm2)

modelData2 <- modelData%>%
  add_predicted_draws(Mglm2, fun = exp, n = 100, seed = 123, re_formula = NULL, category = ".category")

#main hh_level predictors
hh_posterior_fig <- bayesplot::mcmc_areas(posterior, pars = c( "FishSumEdible1000", "LargeGameSumEdible1000", "sms","boats","atvs","cars","(Intercept)"),prob = 0.8) +   ggplot2::scale_y_discrete(labels = c( "Fish harvest 1000lbs", "Game harvest 1000lbs","Snowmobiles","Boats","ATVs", "Cars & trucks","Intercept")) + 
  ggplot2::xlab("Posterior coefficient estimate")+
  ggplot2::ylab("Density")+
  ggplot2::theme_update(text = element_text(size=7)) + # sets text size to points, 7pts required by Elsevier
  ggplot2::geom_line(size=0.01)
hh_posterior_fig
ggsave("hh_posterior_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)

#plotting predictive bands alongside the data
pred_bands <- ggplot(modelData2, aes(y = commname, x = .prediction)) +
  stat_intervalh() +
  geom_point(aes(x = an_gal_total), data = modelData) +
  scale_color_brewer()+
  ylab("")+
  xlab("annual gasoline consumption (gal/household)") + xlim(0,8000) +
  theme(legend.position = c(0.7, 0.9),legend.justification = c(0, 1),legend.text=element_text(size=7),text = element_text(size=7) )+
  labs(color="Percentile of posterior \npredictive distribution")
pred_bands
ggsave("pred_bands_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 140, height = 70, units = "mm",
  dpi = 300, limitsize = TRUE)

#gasoline price predictor by community, probably not needed as we are not interested in the coefficient estimates as much as overall prediction accuracy
price_posterior_fig <- mcmc_areas(posterior, pars = c("b[gasolinePrice commname:Dry_Creek]","b[gasolinePrice commname:Bettles]","b[gasolinePrice commname:Tok]","b[gasolinePrice commname:Dot_Lake]","b[gasolinePrice commname:Evansville]","b[gasolinePrice commname:Anaktuvuk_Pass]","b[gasolinePrice commname:Wiseman]","b[gasolinePrice commname:Alatna]","b[gasolinePrice commname:Beaver]","b[gasolinePrice commname:Allakaket]"),prob = 0.8) +
  ggplot2::scale_y_discrete(labels = c( "Dry Creek","Bettles", "Tok","Dot Lake","Evansville","Anaktuvuk Pass", "Wiseman","Alatna","Beaver","Allakaket")) + 
  ggplot2::xlim(-1,+1) + 
  ggplot2::xlab("Posterior coefficient estimate: gasoline price")+
  ggplot2::ylab("Density")+
  ggplot2::geom_line(size=0.01) +
  #ggplot2::geom_segment(aes(x = 0.8, y = 7, xend = 0.8, yend = 8),arrow = arrow(length = unit(0.3, "cm"))) +
  #ggplot2::annotate("text", x = 0.82, y = 6.5, label = "higher Ak Native %",size=1.5) +
  #ggplot2::geom_segment(aes(x = 0.8, y = 4, xend = 0.8, yend = 3),arrow = arrow(length = unit(0.3, "cm")))+
  #ggplot2::annotate("text", x = 0.82, y = 4.5, label = "lower Ak Native %",size=1.5) +
 ggplot2::theme_update(text = element_text(size=7))  # sets text size to points, 7pts required by Elsevier
price_posterior_fig
ggsave("price_posterior_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 140, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```


##Linear model
Following https://mc-stan.org/users/documentation/case-studies/tutorial_rstanarm.html#model-3-varying-intercept-and-slope-model-with-a-single-predictor
```{r}
library(lme4)
Mlinear <- lmer(formula = gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars +   (1 + gasolinePrice | commname), data = modelData, REML = FALSE)
summary(Mlinear)
isSingular(Mlinear, tol = 1e-05)
MlinearIntercepts <- ranef(Mlinear)$commname
linearPredict <- as.data.frame(predict(Mlinear))
colnames(linearPredict)[1] <- "linearPredict"
```

Result:
not singular

###Comparing PPC with linear model predictions
```{r}
library(gginnards)
library(stringr)
draws <- as.data.frame(t(posterior_predict(Mglm2, draws = 50)))
comPredict <- cbind(Mglm2$y,draws,linearPredict)

library(reshape2)
data<- melt(comPredict)
data$type <- with(data, ifelse(str_detect(data$variable,"V"),"y HB",ifelse(variable=="Mglm2$y","y","y ML")))

PPC <- ggplot(data,aes(x=value, line=variable, color=type)) + 
  geom_line(alpha=0.5, stat="density", show.legend=TRUE, size=0.3) +
  scale_color_manual(name = "", values=c("y"="blue", "y HB"=  "grey","y ML"=  "red"))+
  theme(legend.position = c(0.7, 0.9),legend.justification = c(0, 1),legend.text=element_text(size=7),text = element_text(size=7) )+
  ylab("Density")+
  xlab("Log(annual gasoline consumption)")
PPC
ggsave("PPC_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```

###Comparing PPC of the simpler model with linear model predictions
```{r}
library(gginnards)
library(stringr)
draws <- as.data.frame(t(posterior_predict(Mglm2, draws = 50)))
comPredict <- cbind(Mglm2$y,draws,linearPredict)

library(reshape2)
data<- melt(comPredict)
data$type <- with(data, ifelse(str_detect(data$variable,"V"),"y HB",ifelse(variable=="Mglm2$y","y","y ML")))

PPC <- ggplot(data,aes(x=value, line=variable, color=type)) + 
  geom_line(alpha=0.5, stat="density", show.legend=TRUE, size=0.3) +
  scale_color_manual(name = "", values=c("y"="blue", "y HB"=  "grey","y ML"=  "red"))+
  theme(legend.position = c(0.7, 0.9),legend.justification = c(0, 1),legend.text=element_text(size=7),text = element_text(size=7) )+
  ylab("Density")+
  xlab("Log(annual gasoline consumption)")
PPC
ggsave("PPC_fig_model2.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```

#calculating sampel size per community
```{r}
samples <- data%>%
  group_by(commname, road)%>%
  summarise(n = n_distinct(ID))
modeled <- modelData%>%
  group_by(commname)%>%
  count()
samples <- samples%>%
  left_join(modeled,by="commname")

#community characteristics, median edible lbs by species
harvestmedian <- modelData%>%
  mutate(edible = (FishSumEdible1000+LargeGameSumEdible1000)*1000,
          fishPerc =  FishSumEdible1000*1000/edible,
         gamePerc = LargeGameSumEdible1000*1000/edible)%>%
    summarise(fishPercMean = mean(fishPerc),
            gamePercMean = mean(gamePerc),
            edibleMedian = median(edible),
            atv = mean(atvs),
            sm = mean(sms),
            boat = mean(boats),
            car = mean(cars))
```

#Validation 
We used k=10-fold cross-validation to compare the two models above. 
Following http://mc-stan.org/rstanarm/reference/loo.stanreg.html#k-fold-cv
Results will be ordered by expected predictive accuracy. 
```{r}
kfoldHB1 <- kfold(Mglm, K=10)
kfoldHB2 <- kfold(Mglm2, K=10)
kfoldHB3 <- kfold(Mglm3, K=10)
compare_models(kfoldHB1, kfoldHB2, kfoldHB3, detail=TRUE) #negative 'elpd_diff' favors 1st model, positive favors 2nd
```
Result:
Model comparison: 
(ordered by highest ELPD)

      elpd_diff se_diff
Mglm   0.0       0.0   
Mglm2 -1.3       1.9   
Mglm3 -8.4       4.5 