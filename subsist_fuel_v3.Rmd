---
title: "Subsistence fuel analysis"
author: "Tobias Schwoerer"
date: "June 5 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Data import and exploration
What probability distribution best fits the data?
```{r}
library(dplyr)
library(tidyr)
library(MASS)
library(tidybayes)
library(rstanarm)
library(ggplot2)
library(car)

#data <- read.csv("C:/Users/Toby/Dropbox/DATA/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/Subset4R.csv",stringsAsFactor=FALSE)
data <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/Subset4R.csv",stringsAsFactor=FALSE)

#eliminating missing data, equipment not used, chainsaws, and generators
data2 <- subset(data, !is.na(an_gal))
data2 <- subset(data2, used=="Used this equipment")

data2 <- data2%>%
  filter(resname!="Chainsaw" & resname!="Generator")

#grouping data: each row = one distinct household
hh_data <- data2%>%
  group_by(ID)%>%
  mutate(an_gal_total=sum(an_gal),
         usenum_total=sum(usenum),
         sms = sum(sm),
         atvs = sum(atv),
         cars =sum(car),
         boats = sum(boat))%>%
  subset(!is.na(price), select=-c(an_gal, survey_id, UniqueID, resname, fuel, saw, sm, atv, car, boat, vehicles, usenum))%>%
  distinct(ID, .keep_all = TRUE)

#eliminating records that are below 1gal in gasoline consumption. 
hh_data2 <- hh_data%>%
  subset( an_gal_total>1)

#check whether outcome variable is normally or log-normally distributed
qqp(hh_data2$an_gal_total, "norm")
qqp(hh_data2$an_gal_total, "lnorm")

#eliminating extreme outliers based on distribution fit plot, eliminated households using more than 5000 gallons and harvesting more than 8000lbs.
hh_data3 <- hh_data2%>%
  filter(sum_edible_wei_lbs < 8000 & an_gal_total < 5000 )
#& commname!="Dot Lake"
#recheck log-normal distribution
qqp(hh_data3$an_gal_total, "lnorm")

#rescaling weight data into 1000s of lbs, 
hh_data3 <- hh_data3%>%
  mutate(FishSumEdible1000 = FishSumEdible/1000,
         LargeGameSumEdible1000 = LargeGameSumEdible/1000)
```
#eliminating old price columns, adding new column with updated gasoline prices
Source: 
Department of Commerce Community and Economic Development Division of Community and Regional Affairs Research and Analysis
https://www.commerce.alaska.gov/web/Portals/4/pub/FuelWatch_2011_June.pdf
```{r}
gasoline <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/gasoline_prices.csv",stringsAsFactor=FALSE)
hh_data3 <- hh_data3%>%
  left_join(gasoline, by="commname")
```
Result: 
an_gal_total (annual gasoline consumption in gallons per household) is log-normally distributed, need to transform (https://github.com/stan-dev/rstanarm/issues/115). Data such as the gasoline consumption are often naturally log-normally distributed:  values are often low, but are occasionally high or very high (https://rcompanion.org/handbook/I_12.html)

##Transforming outcome variable
```{r}
#log-transforming an_gal_total
hh_data3$gal_log <- log(hh_data3$an_gal_total)
#Checking normal distribution fit
qqp(hh_data3$gal_log, "norm")
```

#Model estimation
##Bayesian Generalized Linear Models With Group-Specific Terms Via Stan
Fitting a lognormal data model using the log-transformed outcome variable, gal_log. 
Variables that ended up close to zero and not adding any predictive power were: OtherSumEdible, SmallGameSumEdible, VegetationSumEdible, and LargeGameSumDist. 
###Preparing data for model
```{r}
#dropping other price columns and eliminating variables we don't need
keep <- c("an_gal_total", "gal_log", "FishSumEdible1000", "LargeGameSumEdible1000", "usenum_total", "atvs", "sms", "boats", "cars", "gasolinePrice", "commname")
modelData <- hh_data3[keep]
modelData <- modelData%>%
  na.omit()
```

###Testing for homogeneity between communities, which exists if the p-value of the test is less than the significance level of 0.05. 
In order to get valid parameter estimates, we need more or less a homogeneous response variable across groups (communities). See: Gabry and Goodrich 2018: https://cran.r-project.org/web/packages/rstanarm/vignettes/glmer.html  "An analysis that disregards between-group heterogeneity can yield parameter estimates that are wrong if there is between-group heterogeneity but would be relatively precise if there actually were no between-group heterogeneity."

Since the log-transformed data is not exactly normally distributed, we use the Levene test as a more robust alternative to test for the above. 

H0: the group variances are equal (called homogeneity of variance or homoscedasticity)
If the resulting p-value of Levene's test is less than some significance level (typically 0.05), the obtained differences in sample variances are unlikely to have occurred based on random sampling from a population with equal variances. Thus, the null hypothesis of equal variances is rejected and it is concluded that there is a difference between the variances in the population. In other words, there is heteroscedasticity. 
```{r}
library(car)
#Bartlett's test 
bartlett.test(gal_log ~ commname, data = modelData)
# Levene's test with one independent variable. The Levene test, is a more robust alternative to the Bartlett test when the distributions of the data are non-normal.
leveneTest(gal_log ~ commname, data = modelData)

fligner.test(gal_log ~ commname, data = modelData)
```
Result:
In both tests, the H0 is rejected, heterogeneous variances exists between communities. Concluding our estimators are biased. 

Bayesian estimation can help in this case:
"Group-by-group analyses, on the other hand, are valid but produces estimates that are relatively imprecise. While complete pooling or no pooling of data across groups is sometimes called for, models that ignore the grouping structures in the data tend to underfit or overfit (Gelman et al.,2013). Hierarchical modeling provides a compromise by allowing parameters to vary by group at lower levels of the hierarchy while estimating common parameters at higher levels. Inference for each group-level parameter is informed not only by the group-specific information contained in the data but also by the data for other groups as well. This is commonly referred to as borrowing strength or shrinkage." (Gabry and Goodrich 2018)

###model
```{r}
Mglm <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars +   (1 + gasolinePrice | commname), data = modelData, family = gaussian(link = "identity"))

#model summary output
prior_summary(object = Mglm)
Mglm
summary(Mglm)

#Evaluating model convergence and diagnose model through STAN online portal
plot(Mglm, "rhat")
plot(Mglm, "ess")
launch_shinystan(Mglm)
```

##Posterior predictions
```{r}
mdata2 <- modelData%>%
  add_predicted_draws(Mglm, fun = exp, n = 100, seed = 123, re_formula = NULL, category = ".category")
mdata3 <- mdata2%>%
  subset(select=-c(.chain, .iteration))%>%
  group_by(ID)%>%
  summarise(draws_mean = mean(.prediction),
            draws_2.5 = quantile(.prediction, .025),
            draws_97.5 = quantile(.prediction, 0.975),
            an_gal_total = mean(an_gal_total))
#adding community name back in
mdata3$commname <- gsub( "_.*$", "", mdata3$ID )


#Scatterplot figures, showing mean prediction versus data for each community
library(ggplot2)
predplot <- ggplot(mdata3, aes(x=draws_mean,y=an_gal_total)) + facet_wrap(~commname,ncol = 3, scales = "fixed") + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1)
predplot

#plotting predictive bands alongside the data
pred_bands <- ggplot(mdata2, aes(y = commname, x = .prediction)) +
  stat_intervalh() +
  geom_point(aes(x = an_gal_total), data = mdata2) +
  scale_color_brewer()+
  ylab("community") +xlab("predicted gal/year per household") + xlim(0,10000)
pred_bands
```


##Linear model
Following https://mc-stan.org/users/documentation/case-studies/tutorial_rstanarm.html#model-3-varying-intercept-and-slope-model-with-a-single-predictor
```{r}
library(lme4)
Mlinear <- lmer(formula = gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars +   (1 + gasolinePrice | commname), data = modelData, REML = FALSE)
summary(Mlinear)
isSingular(Mlinear, tol = 1e-05)
MlinearIntercepts <- ranef(Mlinear)$commname
```
Result:
boundary (singular) fit, see isSinglular, the test for singularity is positive



#calculating sampel size per community
```{r}
samples <- data%>%
  group_by(commname, road)%>%
  summarise(n = n_distinct(ID))
modeled <- modelData%>%
  group_by(commname)%>%
  count()
samples <- samples%>%
  left_join(modeled,by="commname")

#community characteristics, median edible lbs by species
harvestmedian <- modelData%>%
  group_by(commname)%>%
    summarise(fish = median(FishSumEdible1000)*1000,
            game = median(LargeGameSumEdible1000)*1000,
            atv = mean(atvs),
            sm = mean(sms),
            boat = mean(boats),
            car = mean(cars))
```
