---
title: "Subsistence fuel analysis"
author: "Tobias Schwoerer"
date: "June 5 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Data import and exploration
What probability distribution best fits the data?
```{r}
library(dplyr)
library(tidyr)
library(MASS)
library(tidybayes)
library(rstanarm)
library(ggplot2)
library(car)

#data <- read.csv("C:/Users/Toby/Dropbox/DATA/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/Subset4R.csv",stringsAsFactor=FALSE)
data <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/Subset4R.csv",stringsAsFactor=FALSE)

#eliminating missing data, equipment not used, chainsaws, and generators
data2 <- subset(data, !is.na(an_gal))
data2 <- subset(data2, used=="Used this equipment")

data2 <- data2%>%
  filter(resname!="Chainsaw" & resname!="Generator")

#grouping data: each row = one distinct household
hh_data <- data2%>%
  group_by(ID)%>%
  mutate(an_gal_total=sum(an_gal),
         usenum_total=sum(usenum),
         sms = sum(sm),
         atvs = sum(atv),
         cars =sum(car),
         boats = sum(boat))%>%
  subset(!is.na(price), select=-c(an_gal, survey_id, UniqueID, resname, fuel, saw, sm, atv, car, boat, vehicles, usenum))%>%
  distinct(ID, .keep_all = TRUE)

#eliminating records that are below 1gal in gasoline consumption. 
hh_data2 <- hh_data%>%
  subset( an_gal_total>1)

#check whether outcome variable is normally or log-normally distributed
qqp(hh_data2$an_gal_total, "norm")
qqp(hh_data2$an_gal_total, "lnorm")

#eliminating extreme outliers based on distribution fit plot, eliminated households using more than 5000 gallons and harvesting more than 8000lbs.
hh_data3 <- hh_data2%>%
  filter(sum_edible_wei_lbs < 8000 & an_gal_total < 5000 )
#& commname!="Dot Lake"
#recheck log-normal distribution
qqp(hh_data3$an_gal_total, "lnorm")

#rescaling weight data into 1000s of lbs, 
hh_data3 <- hh_data3%>%
  mutate(FishSumEdible1000 = FishSumEdible/1000,
         LargeGameSumEdible1000 = LargeGameSumEdible/1000)
```
Result: 
an_gal_total (annual gasoline consumption in gallons per household) is log-normally distributed, need to transform (https://github.com/stan-dev/rstanarm/issues/115). Data such as the gasoline consumption are often naturally log-normally distributed:  values are often low, but are occasionally high or very high (https://rcompanion.org/handbook/I_12.html)

##Transforming outcome variable
```{r}
#log-transforming an_gal_total
hh_data3$gal_log <- log(hh_data3$an_gal_total)
#Checking normal distribution fit
qqp(hh_data3$gal_log, "norm")
```

#Model estimation
##Bayesian Generalized Linear Models With Group-Specific Terms Via Stan
Fitting a lognormal data model using the log-transformed outcome variable, gal_log. 
Variables that ended up close to zero and not adding any predictive power were: OtherSumEdible, SmallGameSumEdible, VegetationSumEdible, and LargeGameSumDist. 
```{r}
#eliminating variables we don't need and that contain NAs
modelData <- hh_data3%>%
  select(c(an_gal_total, gal_log, FishSumEdible1000, LargeGameSumEdible1000, usenum_total, atvs, sms, boats, cars, comm_fuel_price, commname))%>%
  na.omit()

#model
Mglm <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars +   (1 + comm_fuel_price | commname), data = modelData, family = gaussian(link = "identity"))

#model summary output
prior_summary(object = Mglm)
Mglm
summary(Mglm)

#Evaluating model convergence and diagnose model through STAN online portal
plot(Mglm, "rhat")
plot(Mglm, "ess")
launch_shinystan(Mglm)
```

##Posterior predictions
```{r}
mdata2 <- modelData%>%
  add_predicted_draws(Mglm, fun = exp, n = 100, seed = 123, re_formula = NULL, category = ".category")
mdata3 <- mdata2%>%
  subset(select=-c(.chain, .iteration))%>%
  group_by(ID)%>%
  summarise(draws_mean = mean(.prediction),
            draws_2.5 = quantile(.prediction, .025),
            draws_97.5 = quantile(.prediction, 0.975),
            an_gal_total = mean(an_gal_total))
#adding community name back in
mdata3$commname <- gsub( "_.*$", "", mdata3$ID )


#Scatterplot figures, showing mean prediction versus data for each community
library(ggplot2)
predplot <- ggplot(mdata3, aes(x=draws_mean,y=an_gal_total)) + facet_wrap(~commname,ncol = 3, scales = "fixed") + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1)
predplot

#plotting predictive bands alongside the data
pred_bands <- ggplot(mdata2, aes(y = commname, x = .prediction)) +
  stat_intervalh() +
  geom_point(aes(x = an_gal_total), data = mdata2) +
  scale_color_brewer()+
  ylab("community") +xlab("predicted gal/year per household") + xlim(0,10000)
pred_bands
```

#calculating sampel size per community
```{r}
samples <- data%>%
  group_by(commname, road)%>%
  summarise(n = n_distinct(ID))
modeled <- modelData%>%
  group_by(commname)%>%
  summarise(n = n_distinct(ID))
samples <- samples%>%
  left_join(modeled,by="commname")
```
