---
title: "Subsistence fuel analysis"
author: "Tobias Schwoerer"
date: "June 5 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Data import and exploration
What probability distribution best fits the data?
```{r}
library(dplyr)
library(tidyr)
library(MASS)
library(tidybayes)
library(rstanarm)
library(ggplot2)
library(car)


#data <- read.csv("C:/Users/Toby/Dropbox/DATA/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/Subset4R.csv",stringsAsFactor=FALSE)
data <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/Subset4R.csv",stringsAsFactor=FALSE)
incdata <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/subdataamblerprojfinal.csv",stringsAsFactor=FALSE)

#Creating consistent ID for other dataset
incdata$ID <- paste(incdata$commname,incdata$hhid, sep="_")
#adding hh size and income to my data
myvars <- c("ID", "hhincadj", "HHSIZE")
newdata <- incdata[myvars]

#adding newdata to data
data <- data%>%
  inner_join(newdata, by="ID")

#eliminating missing data, equipment not used, chainsaws, and generators
data2 <- subset(data, !is.na(an_gal))
data2 <- subset(data2, used=="Used this equipment")

data2 <- data2%>%
  filter(resname!="Chainsaw" & resname!="Generator")

#grouping data: each row = one distinct household
hh_data <- data2%>%
  group_by(ID)%>%
  mutate(an_gal_total=sum(an_gal),
         usenum_total=sum(usenum),
         sms = sum(sm),
         atvs = sum(atv),
         cars =sum(car),
         boats = sum(boat))%>%
  subset(!is.na(price), select=-c(an_gal, survey_id, UniqueID, resname, fuel, saw, sm, atv, car, boat, vehicles, usenum))%>%
  distinct(ID, .keep_all = TRUE)

#eliminating records that are below 1gal in gasoline consumption. 
hh_data2 <- hh_data%>%
  subset( an_gal_total>1)

#check whether outcome variable is normally or log-normally distributed
Aplot <- car::qqPlot(hh_data2$an_gal_total, xlab="normal quantiles", ylab="gasoline consumption", envelope=0.95, id=FALSE)
Bplot <- qqPlot(hh_data2$an_gal_total, "lnorm", xlab="log-normal quantiles", ylab="gasoline consumption", envelope=0.95, id=FALSE)


#eliminating extreme outliers based on distribution fit plot, eliminated households using more than 5000 gallons and harvesting more than 8000lbs.
hh_data3 <- hh_data2%>%
  filter(sum_edible_wei_lbs < 8000 & an_gal_total < 5000 )
#& commname!="Dot Lake"
#recheck log-normal distribution
Cplot <- qqp(hh_data3$an_gal_total, "lnorm", xlab="log-normal quantiles", ylab="gasoline consumption", envelope=0.95, id=FALSE)


#rescaling weight data into 1000s of lbs, 
hh_data3 <- hh_data3%>%
  mutate(FishSumEdible1000 = FishSumEdible/1000,
         LargeGameSumEdible1000 = LargeGameSumEdible/1000)
```
#eliminating old price columns, adding new column with updated gasoline prices
Source: 
Department of Commerce Community and Economic Development Division of Community and Regional Affairs Research and Analysis
https://www.commerce.alaska.gov/web/Portals/4/pub/FuelWatch_2011_June.pdf
```{r}
gasoline <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/gasoline_prices.csv",stringsAsFactor=FALSE)
hh_data3 <- hh_data3%>%
  left_join(gasoline, by="commname")
```
Result: 
an_gal_total (annual gasoline consumption in gallons per household) is log-normally distributed, need to transform (https://github.com/stan-dev/rstanarm/issues/115). Data such as the gasoline consumption are often naturally log-normally distributed:  values are often low, but are occasionally high or very high (https://rcompanion.org/handbook/I_12.html)

##Transformations
```{r}
#log-transforming an_gal_total
hh_data3$gal_log <- log(hh_data3$an_gal_total)
#Checking normal distribution fit
Dplot <- qqp(hh_data3$gal_log, "norm", xlab="normal quantiles", ylab="log(gasoline consumption)",envelope=0.95, id=FALSE)

#log-transforming income variable
hh_data3$hhincadj_log <-ifelse(hh_data3$hhincadj==0,0,log(hh_data3$hhincadj))
```

#Model estimation
##Bayesian Generalized Linear Models With Group-Specific Terms Via Stan
Fitting a lognormal data model using the log-transformed outcome variable, gal_log. 
Variables that ended up close to zero and not adding any predictive power were: OtherSumEdible, SmallGameSumEdible, VegetationSumEdible, and LargeGameSumDist. 
###Preparing data for model
```{r}
#dropping other price columns and eliminating variables we don't need
keep <- c("an_gal_total", "gal_log", "FishSumEdible1000", "LargeGameSumEdible1000", "usenum_total", "atvs", "sms", "boats", "cars", "gasolinePrice", "commname","HHSIZE","hhincadj_log")
modelData <- hh_data3[keep]
modelData <- modelData%>%
  na.omit()
```

#Community characteristics
```{r}
summary <- modelData%>%
   mutate(hhincadj=exp(hhincadj_log))%>%
  group_by(commname)%>%
  summarise(ATV = mean(atvs),
            SN = mean(sms),
            CAR = mean(cars),
            BOAT = mean(boats),
            HH = median(HHSIZE),
            income = median(hhincadj))

```

#prior summary providing a concise summary of the priors used
```{r}
prior_summary(Mglm3)

```
Priors for model 'Mglm1' 
------
Intercept (after predictors centered)
 ~ normal(location = 0, scale = 1000)

Coefficients
 ~ normal(location = [0,0,0,...], scale = [100,100,100,...])

Auxiliary (sigma)
 ~ exponential(rate = 1)
     **adjusted scale = 1.29 (adjusted rate = 1/adjusted scale)

Covariance
 ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)




###Testing for homogeneity between communities, which exists if the p-value of the test is less than the significance level of 0.05. 
In order to get valid parameter estimates, we need more or less a homogeneous response variable across groups (communities). See: Gabry and Goodrich 2018: https://cran.r-project.org/web/packages/rstanarm/vignettes/glmer.html  "An analysis that disregards between-group heterogeneity can yield parameter estimates that are wrong if there is between-group heterogeneity but would be relatively precise if there actually were no between-group heterogeneity."

Since the log-transformed data is not exactly normally distributed, we use the Levene test as a more robust alternative to test for the above. 

H0: the group variances are equal (called homogeneity of variance or homoscedasticity)
If the resulting p-value of Levene's test is less than some significance level (typically 0.05), the obtained differences in sample variances are unlikely to have occurred based on random sampling from a population with equal variances. Thus, the null hypothesis of equal variances is rejected and it is concluded that there is a difference between the variances in the population. In other words, there is heteroscedasticity. 
```{r}
library(car)
#Bartlett's test 
bartlett.test(gal_log ~ commname, data = modelData)
# Levene's test with one independent variable. The Levene test, is a more robust alternative to the Bartlett test when the distributions of the data are non-normal.
leveneTest(gal_log ~ commname, data = modelData)

fligner.test(gal_log ~ commname, data = modelData)
```
Result:
In both tests, the H0 is rejected, heterogeneous variances exists between communities. Concluding our estimators are biased. 

Bayesian estimation can help in this case:
"Group-by-group analyses, on the other hand, are valid but produces estimates that are relatively imprecise. While complete pooling or no pooling of data across groups is sometimes called for, models that ignore the grouping structures in the data tend to underfit or overfit (Gelman et al.,2013). Hierarchical modeling provides a compromise by allowing parameters to vary by group at lower levels of the hierarchy while estimating common parameters at higher levels. Inference for each group-level parameter is informed not only by the group-specific information contained in the data but also by the data for other groups as well. This is commonly referred to as borrowing strength or shrinkage." (Gabry and Goodrich 2018)

### HB models
```{r}
#Model HB1
Mglm1 <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars +  (1 | commname), data = modelData, family = gaussian(link = "identity"), prior = normal(location=0,scale=100,autoscale=F),prior_intercept = normal(location=0,scale=1000,autoscale=F), prior_aux = exponential(rate=1))

Mglm2 <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + HHSIZE +  (1 | commname), data = modelData, family = gaussian(link = "identity"), prior = normal(location=0,scale=100,autoscale=F),prior_intercept = normal(location=0,scale=1000,autoscale=F), prior_aux = exponential(rate=1))

Mglm3 <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + HHSIZE + hhincadj_log +  (1 | commname), data = modelData, family = gaussian(link = "identity"),prior = normal(location=0,scale=100,autoscale=F),prior_intercept = normal(location=0,scale=1000,autoscale=F), prior_aux = exponential(rate=1))
```


#Validation 
We used k=10-fold cross-validation to compare the two models above. 
Following http://mc-stan.org/rstanarm/reference/loo.stanreg.html#k-fold-cv
Results will be ordered by expected predictive accuracy. 
```{r}
kfoldHB1 <- kfold(Mglm1, K=10)
kfoldHB2 <- kfold(Mglm2, K=10)
kfoldHB3 <- kfold(Mglm3, K=10)
compare_models(kfoldHB1, kfoldHB2, kfoldHB3,detail=TRUE)
```




#Diagnostics for Model5
##Predictive checks
```{r}
#Evaluating model convergence and diagnose model through STAN online portal
plot(Mglm5, "rhat")
plot(Mglm5, "ess")


#model summary output
prior_summary(object = Mglm5)
Mglm5
summary(Mglm5)
launch_shinystan(Mglm1)


library(bayesplot)
library(rstan)
if (!require("devtools")) {
  install.packages("devtools")
}
devtools::install_github("stan-dev/bayesplot")

#predictive checking
theme_set(theme_cowplot(font_size=7))
color_scheme_set("brightblue")
Tmean <- Mglm5 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm5$y,
             stat = "mean")
Tsd <- Mglm5 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm5$y,
             stat = "sd")
Tmin <- Mglm5 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm5$y,
             stat = "min")
Tmax <- Mglm5 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm5$y,
             stat = "max")
#combining plots
library(cowplot)
theme_set(theme_cowplot(font_size=7))
plot_grid(Tmean, Tsd, Tmin, Tmax, labels = c("A", "B", "C", "D"), ncol = 2)
ggsave("yyrepGrid_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 140, height = 140, units = "mm",
  dpi = 300, limitsize = TRUE)
```


##Linear model with the same predictors as the best performing HB model, in this case HB5
Following https://mc-stan.org/users/documentation/case-studies/tutorial_rstanarm.html#model-3-varying-intercept-and-slope-model-with-a-single-predictor
we estimate maximum likelihood, not REML = restricted maxim likelihood
```{r}
library(lme4)
Mlinear <- lmer(formula = gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars +   (1 | commname), data = modelData, REML = FALSE)

summary(Mlinear)
isSingular(Mlinear, tol = 1e-05)
MlinearIntercepts <- ranef(Mlinear)$commname
linearPredict <- as.data.frame(predict(Mlinear))
colnames(linearPredict)[1] <- "linearPredict"
```

#Linear Models
```{r}
Ml1 <- lmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + HHSIZE + hhincadj_log +  (1 + gasolinePrice | commname), data = modelData, REML = FALSE)

Ml2 <- lmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + HHSIZE + hhincadj_log + (1 + gasolinePrice | commname), data = modelData, REML = FALSE)

Ml3 <- lmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + HHSIZE + hhincadj_log +  (1 | commname), data = modelData, REML = FALSE)

Ml4 <- lmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + HHSIZE +  (1 | commname), data = modelData, REML = FALSE)

Ml5 <- lmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars +   (1 | commname), data = modelData, REML = FALSE)

Ml6 <- lmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + HHSIZE + (1 + gasolinePrice | commname), data = modelData, REML = FALSE)


kfoldMl3 <- kfold(Ml3, K=10)
kfoldMl4 <- kfold(Ml4, K=10)
kfoldMl5 <- kfold(Ml5, K=10)
compare_models(kfoldMl1, kfoldMl2, kfoldMl3, kfoldMl4,kfoldMl5, kfoldMl6, detail=TRUE)
compare_models(kfoldMl1, kfoldMl2, kfoldMl3, kfoldMl4,kfoldMl5, kfoldMl6, detail=TRUE)
```








###Comparing PPC with linear model predictions
```{r}
library(gginnards)
library(stringr)
draws <- as.data.frame(t(posterior_predict(Mglm5, draws = 50)))
comPredict <- cbind(Mglm5$y,draws,linearPredict)

library(reshape2)
data<- melt(comPredict)
data$type <- with(data, ifelse(str_detect(data$variable,"V"),"y HB",ifelse(variable=="Mglm5$y","y","y ML")))

PPC <- ggplot(data,aes(x=value, line=variable, color=type)) + 
  geom_line(alpha=0.5, stat="density", show.legend=TRUE, size=0.3) +
  scale_color_manual(name = "", values=c("y"="blue", "y HB"=  "grey","y ML"=  "red"))+
  theme(legend.position = c(0.7, 0.9),legend.justification = c(0, 1),legend.text=element_text(family="Times New Roman", size=7),text = element_text(family="Times New Roman",size=7) )+
  ylab("Density")+
  xlab("Log(annual gasoline consumption)")
PPC
ggsave("PPC_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```


##Convergence check
```{r}
#convergence check after warm-up
color_scheme_set("mix-blue-red")
con_test_fig <- mcmc_trace(Mglm5, pars = c("FishSumEdible1000", "sigma"),
           facet_args = list(nrow = 2)) +
   ggplot2::theme_update(text = element_text(size=7))  
con_test_fig
ggsave("con_test_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```


##Descriptive data for presentation in the paper
```{r}
descTable <- modelData%>%
  summarise(hhsize = median(HHSIZE),
            meanhhsize = median(HHSIZE))

```


##Posterior distributions for estimators
not as important as we are not interested in marginal contributions of each variable and rather in the predictive capabilities of the models
```{r}
posterior <- as.matrix(Mglm2)

modelData2 <- modelData%>%
  add_predicted_draws(Mglm2, fun = exp, n = 100, seed = 123, re_formula = NULL, category = ".category")

#main hh_level predictors
hh_posterior_fig <- bayesplot::mcmc_areas(posterior, pars = c( "FishSumEdible1000", "LargeGameSumEdible1000", "sms","boats","atvs","cars","(Intercept)"),prob = 0.8) +   ggplot2::scale_y_discrete(labels = c( "Fish harvest 1000lbs", "Game harvest 1000lbs","Snowmobiles","Boats","ATVs", "Cars & trucks","Intercept")) + 
  ggplot2::xlab("Posterior coefficient estimate")+
  ggplot2::ylab("Density")+
  ggplot2::theme_update(text = element_text(size=7)) + # sets text size to points, 7pts required by Elsevier
  ggplot2::geom_line(size=0.01)
hh_posterior_fig
ggsave("hh_posterior_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)

#plotting predictive bands alongside the data
pred_bands <- ggplot(modelData2, aes(y = commname, x = .prediction)) +
  stat_intervalh() +
  geom_point(aes(x = an_gal_total), data = modelData) +
  scale_color_brewer()+
  ylab("")+
  xlab("annual gasoline consumption (gal/household)") + xlim(0,8000) +
  theme(legend.position = c(0.7, 0.9),legend.justification = c(0, 1),legend.text=element_text(size=7),text = element_text(size=7) )+
  labs(color="Percentile of posterior \npredictive distribution")
pred_bands
ggsave("pred_bands_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 140, height = 70, units = "mm",
  dpi = 300, limitsize = TRUE)

#gasoline price predictor by community, probably not needed as we are not interested in the coefficient estimates as much as overall prediction accuracy
price_posterior_fig <- mcmc_areas(posterior, pars = c("b[gasolinePrice commname:Dry_Creek]","b[gasolinePrice commname:Bettles]","b[gasolinePrice commname:Tok]","b[gasolinePrice commname:Dot_Lake]","b[gasolinePrice commname:Evansville]","b[gasolinePrice commname:Anaktuvuk_Pass]","b[gasolinePrice commname:Wiseman]","b[gasolinePrice commname:Alatna]","b[gasolinePrice commname:Beaver]","b[gasolinePrice commname:Allakaket]"),prob = 0.8) +
  ggplot2::scale_y_discrete(labels = c( "Dry Creek","Bettles", "Tok","Dot Lake","Evansville","Anaktuvuk Pass", "Wiseman","Alatna","Beaver","Allakaket")) + 
  ggplot2::xlim(-1,+1) + 
  ggplot2::xlab("Posterior coefficient estimate: gasoline price")+
  ggplot2::ylab("Density")+
  ggplot2::geom_line(size=0.01) +
  #ggplot2::geom_segment(aes(x = 0.8, y = 7, xend = 0.8, yend = 8),arrow = arrow(length = unit(0.3, "cm"))) +
  #ggplot2::annotate("text", x = 0.82, y = 6.5, label = "higher Ak Native %",size=1.5) +
  #ggplot2::geom_segment(aes(x = 0.8, y = 4, xend = 0.8, yend = 3),arrow = arrow(length = unit(0.3, "cm")))+
  #ggplot2::annotate("text", x = 0.82, y = 4.5, label = "lower Ak Native %",size=1.5) +
 ggplot2::theme_update(text = element_text(size=7))  # sets text size to points, 7pts required by Elsevier
price_posterior_fig
ggsave("price_posterior_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 140, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```