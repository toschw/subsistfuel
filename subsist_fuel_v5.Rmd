---
title: "Subsistence fuel analysis"
author: "Tobias Schwoerer"
date: "June 5 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Data import and exploration
What probability distribution best fits the data?
```{r}
library(dplyr)
library(tidyr)
library(MASS)
library(tidybayes)
library(rstanarm)
library(ggplot2)
library(car)
library(cowplot)


#data <- read.csv("C:/Users/Toby/Dropbox/DATA/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/Subset4R.csv",stringsAsFactor=FALSE)
data <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/Subset4R.csv",stringsAsFactor=FALSE)
incdata <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/subdataamblerprojfinal.csv",stringsAsFactor=FALSE)
road_comm <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/road_comm.csv",stringsAsFactor=FALSE)

#Creating consistent ID for other dataset
incdata$ID <- paste(incdata$commname,incdata$hhid, sep="_")
#adding hh size and income to my data
myvars <- c("ID", "hhincadj", "HHSIZE")
newdata <- incdata[myvars]

#adding newdata to data
data <- data%>%
  inner_join(newdata, by="ID")

#eliminating missing data, equipment not used, chainsaws, and generators
data2 <- subset(data, !is.na(an_gal))
data2 <- subset(data2, used=="Used this equipment")

data2 <- data2%>%
  filter(resname!="Chainsaw" & resname!="Generator")

#grouping data: each row = one distinct household
hh_data <- data2%>%
  group_by(ID)%>%
  mutate(an_gal_total=sum(an_gal),
         usenum_total=sum(usenum),
         sms = sum(sm),
         atvs = sum(atv),
         cars =sum(car),
         boats = sum(boat))%>%
  subset(!is.na(price), select=-c(an_gal, survey_id, UniqueID, resname, fuel, saw, sm, atv, car, boat, vehicles, usenum))%>%
  distinct(ID, .keep_all = TRUE)

#eliminating records that are below 1gal in gasoline consumption. 
hh_data2 <- hh_data%>%
  subset( an_gal_total>1)

#check whether outcome variable is normally or log-normally distributed
Aplot <- car::qqPlot(hh_data2$an_gal_total, xlab="normal quantiles", ylab="gasoline consumption", envelope=0.95, id=FALSE)
Bplot <- car::qqPlot(hh_data2$an_gal_total, "lnorm", xlab="log-normal quantiles", ylab="gasoline consumption", envelope=0.95, id=FALSE)


#eliminating extreme outliers based on distribution fit plot, eliminated households using more than 5000 gallons and harvesting more than 8000lbs.
hh_data3 <- hh_data2%>%
  filter(sum_edible_wei_lbs < 8000 & an_gal_total < 5000 )
#& commname!="Dot Lake"
#recheck log-normal distribution
Cplot <- car::qqPlot(hh_data3$an_gal_total, "lnorm", xlab="log-normal quantiles", ylab="gasoline consumption", envelope=0.95, id=FALSE)


#rescaling weight data into 1000s of lbs, 
hh_data3 <- hh_data3%>%
  mutate(FishSumEdible1000 = FishSumEdible/1000,
         LargeGameSumEdible1000 = LargeGameSumEdible/1000)
```
#eliminating old price columns, adding new column with updated gasoline prices
Source: 
Department of Commerce Community and Economic Development Division of Community and Regional Affairs Research and Analysis
https://www.commerce.alaska.gov/web/Portals/4/pub/FuelWatch_2011_June.pdf
```{r}
gasoline <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/gasoline_prices.csv",stringsAsFactor=FALSE)
hh_data3 <- hh_data3%>%
  left_join(gasoline, by="commname")
```
Result: 
an_gal_total (annual gasoline consumption in gallons per household) is log-normally distributed, need to transform (https://github.com/stan-dev/rstanarm/issues/115). Data such as the gasoline consumption are often naturally log-normally distributed:  values are often low, but are occasionally high or very high (https://rcompanion.org/handbook/I_12.html)

##Transformations
```{r}
#log-transforming an_gal_total
hh_data3$gal_log <- log(hh_data3$an_gal_total)
#Checking normal distribution fit
Dplot <- car::qqPlot(hh_data3$gal_log, "norm", xlab="normal quantiles", ylab="log(gasoline consumption)",envelope=0.95, id=FALSE)

#log-transforming income variable
hh_data3$hhincadj_log <-ifelse(hh_data3$hhincadj==0,0,log(hh_data3$hhincadj))
```

#Model estimation
##Bayesian Generalized Linear Models With Group-Specific Terms Via Stan
Fitting a lognormal data model using the log-transformed outcome variable, gal_log. 
Variables that ended up close to zero and not adding any predictive power were: OtherSumEdible, SmallGameSumEdible, VegetationSumEdible, and LargeGameSumDist. 
###Preparing data for model
```{r}
#dropping other price columns and eliminating variables we don't need
keep <- c("an_gal_total", "gal_log", "FishSumEdible1000", "LargeGameSumEdible1000", "usenum_total", "atvs", "sms", "boats", "cars", "gasolinePrice", "commname","HHSIZE","hhincadj_log")
modelData <- hh_data3[keep]
modelData <- modelData%>%
  na.omit()

#adding road connection dummy variable
modelData$road <- with(modelData,ifelse(commname=="Alatna"|commname=="Allakaket"|commname=="Beaver"|commname=="Anaktuvuk Pass",1,0))

```

#Community characteristics
```{r}
summary <- modelData%>%
   mutate(hhincadj=exp(hhincadj_log))%>%
  group_by(commname)%>%
  summarise(ATV = mean(atvs),
            SN = mean(sms),
            CAR = mean(cars),
            BOAT = mean(boats),
            HH = median(HHSIZE),
            income = median(hhincadj),
            gal = median(an_gal_total),
            fish = median(FishSumEdible1000)*1000,
            game = median(LargeGameSumEdible1000)*1000)
```

##Harvest weights and gasoline consumption plotted
```{r}
#labeling efficient households
#modelDataPlot <- modelData
#modelDataPlot$GameLabel <- with(modelDataPlot,ifelse(LargeGameSumEdible1000>3,commname,""))
modelData$efficiency <- with(modelData,ifelse(LargeGameSumEdible1000>3,"efficient",""))
#modelDataPlot$FishLabel <- with(modelDataPlot,ifelse(FishSumEdible1000>1.3 & an_gal_total <2000,commname,""))
modelData$efficiency <- with(modelData,ifelse(FishSumEdible1000>1.3 & an_gal_total <2000,"efficient",""))
modelData$total <- modelData$FishSumEdible1000 *1000 + modelData$LargeGameSumEdible1000 *1000
modelData$lbsGal <- modelData$total/modelData$an_gal_total


#cross tab by community
commSummary <- modelData%>%
  group_by(commname)%>%
  summarise(median = median(lbsGal),
            max = max(lbsGal),
            min = min(lbsGal),
            sd = sd(lbsGal),
            mean = mean(lbsGal),
            cv = sd/mean)
 #estimate probability that household is more than average efficient in their community
modelData2 <- modelData%>%
  left_join(commSummary,by="commname")
modelData2$eff2 <- with(modelData2, ifelse(lbsGal>=median,"higher_than_community_median","lower_than_community_median"))

#sorting and labeling super households
sortData2 <- modelData2[with(modelData2, order(commname, total)),]
sortData3 <-sortData2 %>% 
  group_by(commname) %>% 
  mutate(id = row_number(),
         idMax = max(id),
         idPerc = id/idMax*100,
         cumWeight = cumsum(total),
         cumWeightMax = max(cumsum(total)),
         cumWeightPerc = cumWeight/cumWeightMax*100)
sortData3$superhh <- with(sortData3, ifelse(cumWeightPerc>=30,"yes","no"))

sortData3$superhh2 <- with(sortData3, ifelse(total>=quantile(sortData3$total, probs=0.66),"top_tertile",ifelse((total<quantile(sortData3$total, probs=0.66)) & (total>=quantile(sortData3$total, probs=0.33)), "middle_tertile", "lowest_tertile")))


#Probability that super household is also more efficient than community average
effSummary <- sortData3[sortData3$idMax >5,]%>%  #use this if wanting to limit to four communities with sufficient housholds
#effSummary <- sortData3%>%
  group_by(commname,eff2,superhh2)%>%
  summarise(count = n())%>%
  spread(superhh2, count)
effSummary[is.na(effSummary)] <- 0

library(tidyverse)
effSummary2 <- column_to_rownames(effSummary, var="eff2")
#effSummary2 <- t(effSummary2) #transposing columns to rows and vice versa

effSummary3 <-  effSummary%>%
  group_by(commname)%>%
  mutate(
        lowest = round(lowest_tertile/sum(lowest_tertile),1),
        middle = round(middle_tertile/sum(middle_tertile), 1),
        top = round(top_tertile/sum(top_tertile), 1))

plotData <- effSummary3%>%
  filter(eff2=="higher_than_community_median")%>%
  gather(tertile, probability, lowest, middle, top)

d=data.frame(a=c())
library(ggplot2)
plot3 <- ggplot(plotData, aes(x=commname, y=probability, color=tertile, shape=commname)) +
 scale_color_brewer(type='div', palette=2)+
   #scale_shape_manual() + #values=c(21, 22, 23, 24))+
  geom_jitter(position = position_jitter(0.1), size=5)+
  theme(axis.text=element_text(family="sans",size=7),axis.title=element_text(family="sans",size=7)) +
  ylab("Probability")+ xlab("") +
 labs(color="Harvest tertile") +
 theme(axis.text=element_text(size=7),axis.title=element_text(size=7), legend.position=c(0.85, 0.2),legend.text=element_text(family="sans", size=7),legend.title=element_text(family="sans", size=7)) +
  guides(shape=FALSE, color=guide_legend(reverse = TRUE))#this eliminates the commname variable from having a legend
plot3

ggsave("efficiency_probability.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```

##Looking for evidence of Wolfe's 30/70 rule
```{r}
#plotting communities with less than 5 data points
library(RColorBrewer)
p <- ggplot(data = sortData3[sortData3$idMax >5,], aes(x=idPerc, y=cumWeightPerc, color=eff2, shape=commname)) +
   geom_point()+
   geom_abline(slope=1, linetype="dashed", color = "black", alpha=0.5) +
   scale_x_continuous(name="Percent of community households", limits=c(0, 100), breaks=c(10,30,50,70,90)) +
   scale_y_continuous(name="Percent of community harvest", limits=c(0, 100), breaks=c(10,30,50,70,90)) +
   theme(axis.text=element_text(size=7),axis.title=element_text(size=7),legend.position=c(0.2, 0.8)) +
   labs(color="Energy efficiency of harvesting",shape="Community")+
    scale_color_grey()
   #geom_path(size=0.5, linetype=3)
p 

p2 <- ggplot(data = sortData3[sortData3$idMax >5,], aes(x=idPerc, y=cumWeightPerc, color=superhh2, shape=commname)) +
   geom_point()+
    #geom_path(size=0.5, linetype=3) +
  geom_abline(slope=1, linetype="dashed", color = "black", alpha=0.5) +
  scale_color_brewer()+
   scale_x_continuous(name="Percent of community households", limits=c(0, 100), breaks=c(10,30,50,70,90)) +
   scale_y_continuous(name="Percent of community harvest", limits=c(0, 100), breaks=c(10,30,50,70,90)) +
   theme(axis.text=element_text(size=7),axis.title=element_text(size=7),legend.position=c(0.2, 0.8)) +
   labs(color="Harvest amount (lbs) tertile",shape="Community")
p2 


theme_set(theme_cowplot(font_size=7))
plot_grid(p, p2, labels = c("A", "B"), ncol = 2)
ggsave("Harvest_and_efficiency.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 190, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```



#prior summary providing a concise summary of the priors used
```{r}
prior_summary(Mglm3)

```
Priors for model 'Mglm1' 
------
Intercept (after predictors centered)
 ~ normal(location = 0, scale = 1000)

Coefficients
 ~ normal(location = [0,0,0,...], scale = [100,100,100,...])

Auxiliary (sigma)
 ~ exponential(rate = 1)
     **adjusted scale = 1.29 (adjusted rate = 1/adjusted scale)

Covariance
 ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)




###Testing for homogeneity between communities, which exists if the p-value of the test is less than the significance level of 0.05. 
In order to get valid parameter estimates, we need more or less a homogeneous response variable across groups (communities). See: Gabry and Goodrich 2018: https://cran.r-project.org/web/packages/rstanarm/vignettes/glmer.html  "An analysis that disregards between-group heterogeneity can yield parameter estimates that are wrong if there is between-group heterogeneity but would be relatively precise if there actually were no between-group heterogeneity."

Since the log-transformed data is not exactly normally distributed, we use the Levene test as a more robust alternative to test for the above. 

H0: the group variances are equal (called homogeneity of variance or homoscedasticity)
If the resulting p-value of Levene's test is less than some significance level (typically 0.05), the obtained differences in sample variances are unlikely to have occurred based on random sampling from a population with equal variances. Thus, the null hypothesis of equal variances is rejected and it is concluded that there is a difference between the variances in the population. In other words, there is heteroscedasticity. 
```{r}
library(car)
#Bartlett's test 
bartlett.test(gal_log ~ commname, data = modelData)
# Levene's test with one independent variable. The Levene test, is a more robust alternative to the Bartlett test when the distributions of the data are non-normal.
leveneTest(gal_log ~ commname, data = modelData)

fligner.test(gal_log ~ commname, data = modelData)
```
Result:
In both tests, the H0 is rejected, heterogeneous variances exists between communities. Concluding our estimators are biased. 

Bayesian estimation can help in this case:
"Group-by-group analyses, on the other hand, are valid but produces estimates that are relatively imprecise. While complete pooling or no pooling of data across groups is sometimes called for, models that ignore the grouping structures in the data tend to underfit or overfit (Gelman et al.,2013). Hierarchical modeling provides a compromise by allowing parameters to vary by group at lower levels of the hierarchy while estimating common parameters at higher levels. Inference for each group-level parameter is informed not only by the group-specific information contained in the data but also by the data for other groups as well. This is commonly referred to as borrowing strength or shrinkage." (Gabry and Goodrich 2018)

### HB models
```{r}
#Model HB1
Mglm1 <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + (1 | commname), data = modelData, family = gaussian(link = "identity"), prior = normal(location=0,scale=10,autoscale=F),prior_intercept = normal(location=0,scale=10,autoscale=F), prior_aux = exponential(rate=1))

Mglm2 <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + HHSIZE +  (1 | commname), data = modelData, family = gaussian(link = "identity"), prior = normal(location=0,scale=10,autoscale=F),prior_intercept = normal(location=0,scale=10,autoscale=F), prior_aux = exponential(rate=1), adapt_delta =0.99)

Mglm3 <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + HHSIZE + hhincadj_log +  (1 | commname), data = modelData, family = gaussian(link = "identity"),prior = normal(location=0,scale=10,autoscale=F),prior_intercept = normal(location=0,scale=10,autoscale=F), prior_aux = exponential(rate=1), adapt_delta = 0.99)
```




#Quick model check for test model
```{r}

prior_summary(object = Mglm1)
Mglm1
summary(Mglm1)
launch_shinystan(Mglm1)
```



#Validation 
We used k=10-fold cross-validation to compare the two models above. 
Following http://mc-stan.org/rstanarm/reference/loo.stanreg.html#k-fold-cv
Results will be ordered by expected predictive accuracy. 
```{r}
kfoldHB1 <- kfold(Mglm1, K=10)
kfoldHB2 <- kfold(Mglm2, K=10)
kfoldHB3 <- kfold(Mglm3, K=10)
compare_models(kfoldHB1, kfoldHB2, kfoldHB3,detail=TRUE)
```

Model formulas: 
  Mglm1:  gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000 + atvs + 
	   sms + boats + cars + (1 | commname)
  Mglm2:  gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000 + atvs + 
	   sms + boats + cars + HHSIZE + (1 | commname)
  Mglm3:  gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000 + atvs + 
	   sms + boats + cars + HHSIZE + hhincadj_log + (1 | commname)

Model comparison: 
(ordered by highest ELPD)

      elpd_diff se_diff
Mglm1  0.0       0.0   
Mglm2 -1.3       2.0   
Mglm3 -2.1       2.4  

#Diagnostics for Model1
```{r}
#Evaluating model convergence and diagnose model through STAN online portal
rhat_fig <- plot(Mglm1, "rhat")
ggsave("rhat_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
ess_fig <- plot(Mglm1, "ess")
ggsave("ess_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```

##Predictive checks
```{r}
library(bayesplot)
library(rstan)
if (!require("devtools")) {
  install.packages("devtools")
}
devtools::install_github("stan-dev/bayesplot")

#predictive checking
theme_set(theme_cowplot(font_size=7))
color_scheme_set("brightblue")
Tmean <- Mglm1 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm2$y,
             stat = "mean")
Tsd <- Mglm1 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm2$y,
             stat = "sd")
Tmin <- Mglm1 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm2$y,
             stat = "min")
Tmax <- Mglm1 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm2$y,
             stat = "max")
#combining plots
library(cowplot)
theme_set(theme_cowplot(font_size=7))
plot_grid(Tmean, Tsd, Tmin, Tmax, labels = c("A", "B", "C", "D"), ncol = 2)
ggsave("yyrepGrid_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 140, height = 140, units = "mm",
  dpi = 300, limitsize = TRUE)
```


##Linear model with the same predictors as the best performing HB model, in this case HB5
Following https://mc-stan.org/users/documentation/case-studies/tutorial_rstanarm.html#model-3-varying-intercept-and-slope-model-with-a-single-predictor
we estimate maximum likelihood, not REML = restricted maxim likelihood
```{r}
library(lme4)
Mlinear <- lmer(formula = gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars +   (1 | commname), data = modelData, REML = FALSE)

summary(Mlinear)
isSingular(Mlinear, tol = 1e-05)
MlinearIntercepts <- ranef(Mlinear)$commname
linearPredict <- as.data.frame(predict(Mlinear))
colnames(linearPredict)[1] <- "linearPredict"
```

###Comparing PPC with linear model predictions
```{r}
library(gginnards)
library(stringr)
draws <- as.data.frame(t(posterior_predict(Mglm1, draws = 50)))
comPredict <- cbind(Mglm1$y,draws,linearPredict)

library(reshape2)
data<- melt(comPredict)
data$type <- with(data, ifelse(str_detect(data$variable,"V"),"y HB",ifelse(variable=="Mglm1$y","y","y ML")))

PPC <- ggplot(data,aes(x=value, line=variable, color=type)) + 
  geom_line(alpha=0.5, stat="density", show.legend=TRUE, size=0.3) +
  scale_color_manual(name = "", values=c("y"="blue", "y HB"=  "grey","y ML"=  "red"))+
  theme(legend.position = c(0.7, 0.9),legend.justification = c(0, 1),legend.text=element_text(family="Times New Roman", size=7),text = element_text(family="Times New Roman",size=7) )+
  ylab("Density")+
  xlab("Log(annual gasoline consumption)")
PPC
ggsave("PPC_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```

##Convergence check
```{r}
#convergence check after warm-up
color_scheme_set("mix-blue-red")
con_test_fig <- mcmc_trace(Mglm5, pars = c("FishSumEdible1000", "sigma"),
           facet_args = list(nrow = 2)) +
   ggplot2::theme_update(text = element_text(size=7))  
con_test_fig
ggsave("con_test_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```

##Posterior distributions for estimators
not as important as we are not interested in marginal contributions of each variable and rather in the predictive capabilities of the models
```{r}
posterior <- as.matrix(Mglm1)

modelData2 <- modelData%>%
  add_predicted_draws(Mglm1, fun = exp, n = 100, seed = 123, re_formula = NULL, category = ".category")

#main hh_level predictors
hh_posterior_fig <- bayesplot::mcmc_areas(posterior, 
                                          pars = c("FishSumEdible1000", "LargeGameSumEdible1000", "sms","boats","atvs","cars","(Intercept)"),prob = 0.8) +                 ggplot2::scale_y_discrete(labels = c("Fish harvest 1000lbs", "Game harvest 1000lbs","Snowmobiles","Boats","ATVs", "Cars & trucks","Intercept")) + 
  ggplot2::xlab("Posterior coefficient estimate")+
  ggplot2::ylab("Density")+
  ggplot2::theme_update(text = element_text(size=7)) + # sets text size to points, 7pts required by Elsevier
  ggplot2::geom_line(size=0.01)
hh_posterior_fig
ggsave("hh_posterior_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)

#plotting predictive bands alongside the data
pred_bands <- ggplot(modelData2, aes(y = commname, x = .prediction)) +
  stat_intervalh() +
  geom_point(aes(x = an_gal_total), data = modelData) +
  scale_color_brewer()+
  ylab("")+
  xlab("annual gasoline consumption (gal/household)") + xlim(0,8000) +
  theme(legend.position = c(0.7, 0.9),legend.justification = c(0, 1),legend.text=element_text(size=7),text = element_text(size=7) )+
  labs(color="Percentile of posterior \npredictive distribution")
pred_bands
ggsave("pred_bands_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 140, height = 70, units = "mm",
  dpi = 300, limitsize = TRUE)
```

#Comparing communities in their annual gasoline consumption
E.g. calculating the posterior probability that average gasoline consumption in community A is higher than in community B
```{r}
# Obtain community-level varying intercept beta_0j
# draws for overall mean
mu_a_sims <- as.matrix(Mglm1, 
                       pars = "(Intercept)")
# draws for 10 communities' household-level error
u_sims <- as.matrix(Mglm1, 
                    regex_pars = "b\\[\\(Intercept\\) commname\\:")
# draws for 10 communities' varying intercepts               
a_sims <- as.numeric(mu_a_sims) + u_sims          

# Obtain sigma_y and sigma_alpha^2
# draws for sigma_y
s_y_sims <- as.matrix(Mglm1, 
                       pars = "sigma")
# draws for sigma_alpha^2
s__alpha_sims <- as.matrix(Mglm1, 
                       pars = "Sigma[commname:(Intercept),(Intercept)]")

# Compute mean, SD, median, and 95% credible interval of varying intercepts

# Posterior mean and SD of each alpha
a_mean <- apply(X = a_sims,     # posterior mean
                MARGIN = 2,
                FUN = mean)
a_sd <- apply(X = a_sims,       # posterior SD
              MARGIN = 2,
              FUN = sd)

# Posterior median and 95% credible interval
a_quant <- apply(X = a_sims, 
                 MARGIN = 2, 
                 FUN = quantile, 
                 probs = c(0.025, 0.50, 0.975))
a_quant <- data.frame(t(a_quant))
names(a_quant) <- c("Q2.5", "Q50", "Q97.5")

# Combine summary statistics of posterior simulation draws
a_df <- data.frame(a_mean, a_sd, a_quant)
#converting back to annual gallons, exponentiate
a_df_exp <- a_df%>%
  tibble::rownames_to_column('commname') #this keeps rownames preserved in code thereafter
a_df_exp2 <- a_df_exp%>%
  dplyr::mutate(a_mean = exp(a_mean),
                   a_sd = exp(a_sd),
                   Q2.5 = exp(Q2.5),
                 Q50 = exp(Q50),
                 Q97.5 = exp(Q97.5))
a_df_exp2$commname <- sub('.*:', '', a_df_exp2$commname)
a_df_exp2$commname <- gsub('.$', '', a_df_exp2$commname)
```

#Caterpillar plot of community varying intercepts with 95% credible intervals
```{r}
#specify order for ggplot to recognize
a_df_exp2$commname <- factor(a_df_exp2$commname, levels = a_df_exp2$commname[order(a_df_exp2$a_mean)])

# Plot community-level beta zero's posterior mean and 95% credible interval
beta_zeros <- ggplot(a_df_exp2, aes(x = commname, y = a_mean))+
  geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5)) + 
  geom_hline(yintercept = mean(a_df_exp2$a_mean), size = 0.5,col = "red") + 
  scale_y_continuous(expression(paste("gallons, ", beta["0j"]))) + 
  theme(axis.text.x = element_text(angle = 45, vjust=0.5, size=7), axis.title.x=element_blank(), axis.text.y = element_text(size=7), axis.title.y = element_text(size=7) )
beta_zeros
          
```


#Interpretation of model summary output for household-level coefficients
Since only the response variable was log-transformed, we can exponentiate each  coefficient subtract 1 and multiply by 100 to get the response of every one-unit increase in the independent variable. https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/
```{r}
Mglm1_df <- as.data.frame(summary(Mglm1))
#change columnnames for better reference below
Mglm1_df2 <- Mglm1_df%>%
  tibble::rownames_to_column('variable') #this keeps rownames preserved in code thereafter
colnames(Mglm1_df2) <- c("variable", "mean",  "mcse",  "sd",    "Q2.5",  "Q25",   "Q50",   "Q75",   "Q97.5", "n_eff", "Rhat")

#expoentiate for better interpretation of the coefficients
Mglm1_df2 <- Mglm1_df2%>%
  mutate(mean_exp = (exp(mean)-1)*100,
         Q2.5_exp = (exp(Q2.5)-1)*100,
         Q97.5_exp = (exp(Q97.5)-1)*100)
#eliminate rows related with intercepts
keep <- c("FishSumEdible1000","LargeGameSumEdible1000","atvs","sms","boats","cars") 
Mglm1_df3 <- Mglm1_df2%>%
  filter(variable %in% keep)

```

#Ploting betas and combining with beta zeros from above
```{r}
#specify order for ggplot to recognize
Mglm1_df3$variable <- factor(Mglm1_df3$variable, levels = Mglm1_df3$variable[order(Mglm1_df3$mean_exp)])
# Plot community-level beta zero's posterior mean and 95% credible interval
betas <- ggplot(data = Mglm1_df3, 
       aes(x = variable, 
           y = mean_exp)) +
  geom_pointrange(aes(ymin = Q2.5_exp, 
                      ymax = Q97.5_exp)) + 
  scale_y_continuous(expression(paste("% change," , beta[]))) + 
  theme(axis.text.x = element_text(angle = 45, vjust=0.5, size=7), axis.title.x=element_blank(), axis.text.y = element_text(size=7), axis.title.y = element_text(size=7))+
  scale_x_discrete(labels = c("Fish harvest 1000lbs", "Game harvest 1000lbs","Snowmobiles","Boats","ATVs", "Cars & trucks"))
betas

library(cowplot)
theme_set(theme_cowplot(font_size=7))
plot_grid(beta_zeros, betas, labels = c("A", "B"), ncol = 2, align = "h", axis = "bt", rel_widths = c(1.5, 1) )

ggsave("betas.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 140, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```



PARKING LOT
```{r}
GamePlot <- ggplot() +
 geom_point(data = sortData3[sortData3$idMax >5,], aes(x=LargeGameSumEdible1000, y=an_gal_total, color=commname))+
  geom_text(hjust=1.01, vjust=.1, size=3) +
  xlab("Large game harvest, 1000s of lbs")+
  ylab("gallons")+
    scale_x_continuous(limits = c(0, 8)) +
    jitter()+
theme(axis.text=element_text(size=7),axis.title=element_text(size=7),legend.position="none")
GamePlot

FishPlot <- ggplot() +
 geom_point(data = sortData3[sortData3$idMax >5,], aes(x=FishSumEdible1000, y=an_gal_total, color=commname))+
  geom_text(hjust=1.01, vjust=.1, size=3) +
  xlab("Fish harvest, 1000s of lbs")+
  ylab("gallons")+
  geom_jitter() +
  scale_x_continuous(limits = c(0, 8)) +
theme(axis.text=element_text(size=7),axis.title=element_text(size=7),legend.position=c(0.8, 0.8),legend.title = element_blank())
FishPlot

theme_set(theme_cowplot(font_size=7))
plot_grid(GamePlot, FishPlot, labels = c("A", "B"), ncol = 2)
ggsave("Harvest_fig2.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 190, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)
```
