---
title: "Predicting the food-energy nexus of wild food systems"
author: "Tobias Schwoerer"
date: "November 5 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Data import, preparation and setting global theme for figures
```{r}
#loading libraries
library(dplyr)
library(tidyr)
library(MASS)
library(tidybayes)
library(rstanarm)
library(ggplot2)
library(car)
library(cowplot)
library(tidyverse)
library(RColorBrewer)
library(bayesplot)
library(rstan)
library(car)

#Setting global theme for figures
theme_set(theme_classic(base_size = 9))

#Importing data archived at http://doi.org/10.5063/F1PK0DGR
modelData <- read.csv("https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Aec1bb1ef-6aee-42a6-9d2f-ee4f9fda7851")
```

#Fig A.1
Checking normality assumption of the outcome variable
```{r}
Aplot <- car::qqPlot(modelData$an_gal_total, xlab="normal quantiles", ylab="gasoline consumption", envelope=0.95, id=FALSE)
Bplot <- car::qqPlot(modelData$an_gal_total, "lnorm", xlab="log-normal quantiles", ylab="gasoline consumption", envelope=0.95, id=FALSE)
Cplot <- car::qqPlot(modelData$gal_log, "norm", xlab="normal quantiles", ylab="log(gasoline consumption)",envelope=0.95, id=FALSE)
```

#Table 3 Household characteristics
```{r}
modelData$total <- modelData$LargeGameSumEdible1000+modelData$FishSumEdible1000
summary <- modelData%>%
  group_by(commname)%>%
  summarise(ATV = mean(atvs),
            SN = mean(sms),
            CAR = mean(cars),
            BOAT = mean(boats),
            gal = median(an_gal_total),
            fish = median(FishSumEdible1000)*1000,
            game = median(LargeGameSumEdible1000)*1000,
            total = median(total)*1000,
            sample_size = n())
```

#Analysis related to harvest efficiency
```{r}
#Harvest efficiency calculations
modelData$efficiency <- with(modelData,ifelse(LargeGameSumEdible1000>3,"efficient",""))
modelData$efficiency <- with(modelData,ifelse(FishSumEdible1000>1.3 & an_gal_total <2000,"efficient",""))
modelData$total <- modelData$FishSumEdible1000 *1000 + modelData$LargeGameSumEdible1000 *1000
modelData$lbsGal <- modelData$total/modelData$an_gal_total

#cross tab by community
commSummary <- modelData%>%
  group_by(commname)%>%
  summarise(median = median(lbsGal),
            max = max(lbsGal),
            min = min(lbsGal),
            sd = sd(lbsGal),
            mean = mean(lbsGal),
            cv = sd/mean)

#estimate probability that household is more than average efficient in their community
modelData2 <- modelData%>%
  left_join(commSummary,by="commname")
modelData2$eff2 <- with(modelData2, ifelse(lbsGal>=median,"higher_than_community_median","lower_than_community_median"))

#sorting and labeling super households
sortData2 <- modelData2[with(modelData2, order(commname, total)),]
sortData3 <-sortData2 %>% 
  group_by(commname) %>% 
  mutate(id = row_number(),
         idMax = max(id),
         idPerc = id/idMax*100,
         cumWeight = cumsum(total),
         cumWeightMax = max(cumsum(total)),
         cumWeightPerc = cumWeight/cumWeightMax*100)
sortData3$superhh <- with(sortData3, ifelse(cumWeightPerc>=30,"yes","no"))

sortData3$superhh2 <- with(sortData3, ifelse(total>=quantile(sortData3$total, probs=0.66),"top_tertile",ifelse((total<quantile(sortData3$total, probs=0.66)) & (total>=quantile(sortData3$total, probs=0.33)), "middle_tertile", "lowest_tertile")))

#Figure Probability that super household is also more efficient than community average
effSummary <- sortData3[sortData3$idMax >5,]%>%  #use this if wanting to limit to four communities with sufficient housholds
#effSummary <- sortData3%>%
  group_by(commname,eff2,superhh2)%>%
  summarise(count = n())%>%
  spread(superhh2, count)
effSummary[is.na(effSummary)] <- 0

effSummary3 <-  effSummary%>%
  group_by(commname)%>%
  mutate(
        lowest = round(lowest_tertile/sum(lowest_tertile),1),
        middle = round(middle_tertile/sum(middle_tertile), 1),
        top = round(top_tertile/sum(top_tertile), 1))

plotData <- effSummary3%>%
  filter(eff2=="higher_than_community_median")%>%
  gather(tertile, probability, lowest, middle, top)
```

#Figure 3
```{r}
Fig3 <- ggplot(plotData, aes(x=commname, y=probability, color=tertile, shape=commname, stroke=1)) +
 scale_shape_manual(values=c(15,17,7,18))+
  scale_color_manual(values = c("#cccccc", "#636363", "#252525"))+
  #scale_colour_brewer(palette = "Set2")+
   #scale_shape_manual() + #values=c(21, 22, 23, 24))+
  geom_jitter(position = position_jitter(0.2), size=5)+
   theme(axis.text=element_text(family="sans",size=9),axis.title=element_text(family="sans",size=9)) +
  ylab("Probability")+ xlab("") +
 labs(color="Harvest tertile") +
 theme(axis.text.x=element_text(angle = 90, vjust=0.5, size=9),axis.title=element_text(size=9), legend.position=c(0.85, 0.22),legend.text=element_text(family="sans", size=7),legend.title=element_text(family="sans", size=7)) +
  guides(shape=FALSE, color=guide_legend(reverse = TRUE))#this eliminates the commname variable from having a legend
Fig3
```

# Figure 2
Looking for evidence of Wolfe's 30/70 rule
```{r}
#plotting communities with more than 5 data points
Fig2 <- ggplot(data = sortData3[sortData3$idMax >5,], aes(x=idPerc, y=cumWeightPerc, color=eff2, shape=commname, stroke=1)) +
   scale_shape_manual(values=c(15,17,7,18)) + 
   
  geom_point(size=2)+
   geom_abline(slope=1, linetype="dashed", color = "black", alpha=0.5) +
   scale_x_continuous(name="Cum. percent of community households", limits=c(0, 100), breaks=c(10,30,50,70,90)) +
   scale_y_continuous(name="Cum. percent of community harvest", limits=c(0, 100), breaks=c(10,30,50,70,90)) +
   #theme(axis.text=element_text(size=7),axis.title=element_text(size=7),legend.position=c(0.2, 0.8)) +
   labs(color="Energy efficiency of harvesting",shape="Community")+
  scale_color_manual(labels = c("higher than community median", "lower than community median"), values = c("black", "grey")) +
  theme_classic() 
Fig2 + geom_segment(aes(x = 70, y = 0, xend = 70, yend = 70))
```

#Homogeneity test
```{r}
#Bartlett's test 
bartlett.test(gal_log ~ commname, data = modelData)
# Levene's test with one independent variable. The Levene test, is a more robust alternative to the Bartlett test when the distributions of the data are non-normal.
leveneTest(gal_log ~ commname, data = modelData)
fligner.test(gal_log ~ commname, data = modelData)
```

#Model estimation
##Bayesian Generalized Linear Models With Group-Specific Terms Via Stan
Fitting a lognormal data model using the log-transformed outcome variable, gal_log. 
```{r}
#Model HB1
Mglm1 <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + (1 | commname), data = modelData, family = gaussian(link = "identity"), prior = normal(location=0,scale=10,autoscale=F),prior_intercept = normal(location=0,scale=10,autoscale=F), prior_aux = exponential(rate=1))

Mglm2 <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + HHSIZE +  (1 | commname), data = modelData, family = gaussian(link = "identity"), prior = normal(location=0,scale=10,autoscale=F),prior_intercept = normal(location=0,scale=10,autoscale=F), prior_aux = exponential(rate=1), adapt_delta =0.99)

Mglm3 <- stan_glmer(gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars + HHSIZE + hhincadj_log +  (1 | commname), data = modelData, family = gaussian(link = "identity"),prior = normal(location=0,scale=10,autoscale=F),prior_intercept = normal(location=0,scale=10,autoscale=F), prior_aux = exponential(rate=1), adapt_delta = 0.99)
```

###Quick model check for test model
```{r}
prior_summary(object = Mglm1)
Mglm1
summary(Mglm1)
launch_shinystan(Mglm1)
```

###Cross-validation 
We used k=10-fold cross-validation to compare models above. 
```{r}
kfoldHB1 <- kfold(Mglm1, K=10)
kfoldHB2 <- kfold(Mglm2, K=10)
kfoldHB3 <- kfold(Mglm3, K=10)
compare_models(kfoldHB1, kfoldHB2, kfoldHB3,detail=TRUE)
```

###Model diagnostic plots
####Convergence 
```{r}
#Evaluating model convergence and diagnose model through STAN online portal
rhat_fig <- plot(Mglm1, "rhat")
ess_fig <- plot(Mglm1, "ess")
```

####Distributions related to test statistics
```{r}
if (!require("devtools")) {
  install.packages("devtools")
}
devtools::install_github("stan-dev/bayesplot")

#predictive checking
theme_set(theme_cowplot(font_size=7))
color_scheme_set("brightblue")
Tmean <- Mglm1 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm2$y,
             stat = "mean")
Tsd <- Mglm1 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm2$y,
             stat = "sd")
Tmin <- Mglm1 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm2$y,
             stat = "min")
Tmax <- Mglm1 %>%
  posterior_predict(draws = 500) %>%
  ppc_stat(y = Mglm2$y,
             stat = "max")
#combining plots
theme_set(theme_cowplot(font_size=7))
plot_grid(Tmean, Tsd, Tmin, Tmax, labels = c("A", "B", "C", "D"), ncol = 2)
```


##Maximum likelihood
```{r}
library(lme4)
Mlinear <- lmer(formula = gal_log ~ 1 + FishSumEdible1000 + LargeGameSumEdible1000   + atvs + sms + boats + cars +   (1 | commname), data = modelData, REML = FALSE)

summary(Mlinear)
isSingular(Mlinear, tol = 1e-05)
MlinearIntercepts <- ranef(Mlinear)$commname
linearPredict <- as.data.frame(predict(Mlinear))
colnames(linearPredict)[1] <- "linearPredict"
```

#Comparing HB and ML estimates with raw data 
```{r}
library(gginnards)
library(stringr)
draws <- as.data.frame(t(posterior_predict(Mglm1, draws = 40)))
comPredict <- cbind(Mglm1$y,draws,linearPredict)

library(reshape2)
dataPlot<- melt(comPredict)
dataPlot$type <- with(dataPlot, ifelse(str_detect(dataPlot$variable,"V"),"y HB",ifelse(variable=="Mglm1$y","y","y ML")))

#changing sort order so that lines that should go on top are at the bottom of the dataframe
dataPlot <- data.table::setorder(data, type)

Fig4 <- ggplot() + 
  scale_color_manual(name = "", values=c( "y HB"=  "#cccccc","y ML"=  "#636363","y"="#252525"))+
  geom_line(alpha=1, stat="density", show.legend=TRUE, size=0.75, data = dataPlot[100:4059,], aes(x = value, line=variable, color="y HB"), size = 0.6) +
  geom_line(linetype = "dashed", alpha=1, stat="density", show.legend=TRUE, size=0.75, data = dataPlot[4060:4158,], aes(x = value, line=variable, color="y ML"), size = 0.6) +
    geom_line(alpha=1, stat="density", show.legend=TRUE, size=0.75, data = dataPlot[1:99,], aes(x = value, line=variable, color="y"), size = 0.6)+
  ylab("Density")+
  xlab("Log(annual gasoline consumption)")+
  theme(legend.position = c(0.87, 0.7))
Fig4
```

#Convergence check for preferred HB model
```{r}
#convergence check after warm-up
color_scheme_set("mix-blue-red")
con_test_fig <- mcmc_trace(Mglm1, pars = c("FishSumEdible1000", "sigma"),
           facet_args = list(nrow = 2)) +
   ggplot2::theme_update(text = element_text(size=7))  
con_test_fig
```

##Posterior predictive distributions against data points by community
```{r}
posterior <- as.matrix(Mglm1)

modelData2 <- modelData%>%
  add_predicted_draws(Mglm1, fun = exp, n = 100, seed = 123, re_formula = NULL, category = ".category")

#main hh_level predictors
hh_posterior_fig <- bayesplot::mcmc_areas(posterior, 
                                          pars = c("FishSumEdible1000", "LargeGameSumEdible1000", "sms","boats","atvs","cars","(Intercept)"),prob = 0.8) +                 ggplot2::scale_y_discrete(labels = c("Fish harvest 1000lbs", "Game harvest 1000lbs","Snowmobiles","Boats","ATVs", "Cars & trucks","Intercept")) + 
  ggplot2::xlab("Posterior coefficient estimate")+
  ggplot2::ylab("Density")+
  ggplot2::theme_update(text = element_text(size=7)) + # sets text size to points, 7pts required by Elsevier
  ggplot2::geom_line(size=0.01)
hh_posterior_fig
ggsave("Figures/hh_posterior_fig.bmp", plot = last_plot(), device = "bmp", path = NULL,
  scale = 1, width = 90, height = 90, units = "mm",
  dpi = 300, limitsize = TRUE)

#plotting predictive bands alongside the data
Fig5 <- ggplot(modelData2, aes(y = commname, x = .prediction)) +
  stat_intervalh() +
  geom_point(aes(x = an_gal_total), data = modelData) +
  scale_color_brewer(palette = "Greys")+
  ylab("")+
  xlab("annual gasoline consumption (gal/household)") + xlim(0,8000) +
  theme(legend.position = c(0.7, 0.9),legend.justification = c(0, 1),legend.text=element_text(size=9),text = element_text(size=9) )+
  labs(color="Percentile of posterior \npredictive distribution")
Fig5

```

#Comparing communities in their annual gasoline consumption
E.g. calculating the posterior probability that average gasoline consumption in community A is higher than in community B
```{r}
# Obtain community-level varying intercept beta_0j
# draws for overall mean
mu_a_sims <- as.matrix(Mglm1, 
                       pars = "(Intercept)")
# draws for 10 communities' household-level error
u_sims <- as.matrix(Mglm1, 
                    regex_pars = "b\\[\\(Intercept\\) commname\\:")
# draws for 10 communities' varying intercepts               
a_sims <- as.numeric(mu_a_sims) + u_sims          

# Obtain sigma_y and sigma_alpha^2
# draws for sigma_y
s_y_sims <- as.matrix(Mglm1, 
                       pars = "sigma")
# draws for sigma_alpha^2
s__alpha_sims <- as.matrix(Mglm1, 
                       pars = "Sigma[commname:(Intercept),(Intercept)]")

# Compute mean, SD, median, and 95% credible interval of varying intercepts

# Posterior mean and SD of each beta_0j
a_mean <- apply(X = a_sims,     # posterior mean
                MARGIN = 2,
                FUN = mean)
a_sd <- apply(X = a_sims,       # posterior SD
              MARGIN = 2,
              FUN = sd)

# Posterior median and 95% credible interval
a_quant <- apply(X = a_sims, 
                 MARGIN = 2, 
                 FUN = quantile, 
                 probs = c(0.025, 0.50, 0.975))
a_quant <- data.frame(t(a_quant))
names(a_quant) <- c("Q2.5", "Q50", "Q97.5")

# Combine summary statistics of posterior simulation draws
a_df <- data.frame(a_mean, a_sd, a_quant)
#converting back to annual gallons, exponentiate
a_df_exp <- a_df%>%
  tibble::rownames_to_column('commname') #this keeps rownames preserved in code thereafter
a_df_exp2 <- a_df_exp%>%
  dplyr::mutate(a_mean = exp(a_mean),
                   a_sd = exp(a_sd),
                   Q2.5 = exp(Q2.5),
                 Q50 = exp(Q50),
                 Q97.5 = exp(Q97.5))
a_df_exp2$commname <- sub('.*:', '', a_df_exp2$commname)
a_df_exp2$commname <- gsub('.$', '', a_df_exp2$commname)
```

#Fig 6A
Caterpillar plot of community varying intercepts with 95% credible intervals
```{r}
#specify order for ggplot to recognize
a_df_exp2$commname <- factor(a_df_exp2$commname, levels = a_df_exp2$commname[order(a_df_exp2$a_mean)])

# Plot community-level beta zero's posterior mean and 95% credible interval
beta_zeros <- ggplot(a_df_exp2, aes(x = commname, y = a_mean))+
   theme_classic()+
  geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5)) + 
  geom_hline(yintercept = mean(a_df_exp2$a_mean), size = 0.5,  linetype="dashed", color = "black", alpha=0.5) + 
  scale_y_continuous(expression(paste("gallons, ", beta["0j"]))) + 
  theme(axis.text.x = element_text(angle = 90, vjust=0.5, size=9), axis.title.x=element_blank(), axis.text.y =   element_text(size=9), axis.title.y = element_text(size=9)) +
  scale_x_discrete(labels = c("Dry Creek", "Tok","Betles","Dot Lake","Evansville","Anaktuvuk Pass","Wiseman","Alatna","Beaver","Allakaket"))
  
beta_zeros 
```

#Fig 6B
Ploting betas and combining with beta zeros from above
THIS NOT WORKING PROPERLY!!
```{r}
#specify order for ggplot to recognize
Mglm1_df3$variable <- factor(Mglm1_df3$variable, levels = Mglm1_df3$variable[order(Mglm1_df3$mean_exp)])
# Plot community-level beta zero's posterior mean and 95% credible interval
betas <- ggplot(data = Mglm1_df3, aes(x = variable, y = mean_exp)) +
  theme_classic() +
  geom_pointrange(aes(ymin = Q2.5_exp, ymax = Q97.5_exp)) + 
  scale_y_continuous(expression(paste("% change," , beta[]))) + 
  theme(axis.text.x = element_text(angle = 90, vjust=0.5, size=9), axis.title.x=element_blank(), axis.text.y =element_text(size=9),axis.title.y =element_text(size=9)) +
  scale_x_discrete(labels = c("Fish harvest 1000lbs", "Game harvest 1000lbs","Snowmobiles","Boats","ATVs", "Cars & trucks"))
betas

theme_set(theme_cowplot(font_size=7))
Fig6 <- plot_grid(beta_zeros, betas, labels = c("A", "B"), ncol = 2, align = "h", axis = "bt", rel_widths = c(1.5, 1) )
Fig6
```

#Interpretation of model summary output for household-level coefficients
Since only the response variable was log-transformed, we can exponentiate each  coefficient subtract 1 and multiply by 100 to get the response of every one-unit increase in the independent variable. https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/
```{r}
Mglm1_df <- as.data.frame(summary(Mglm1))
#change columnnames for better reference below
Mglm1_df2 <- Mglm1_df%>%
  tibble::rownames_to_column('variable') #this keeps rownames preserved in code thereafter
colnames(Mglm1_df2) <- c("variable", "mean",  "mcse",  "sd",    "Q2.5",  "Q25",   "Q50",   "Q75",   "Q97.5", "n_eff", "Rhat")

#expoentiate for better interpretation of the coefficients
Mglm1_df2 <- Mglm1_df2%>%
  mutate(mean_exp = (exp(mean)-1)*100,
         Q2.5_exp = (exp(Q2.5)-1)*100,
         Q97.5_exp = (exp(Q97.5)-1)*100)
#eliminate rows related with intercepts
keep <- c("FishSumEdible1000","LargeGameSumEdible1000","atvs","sms","boats","cars") 
Mglm1_df3 <- Mglm1_df2%>%
  filter(variable %in% keep)
```
