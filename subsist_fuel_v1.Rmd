---
title: "Subsistence fuel analysis"
author: "Tobias Schwoerer"
date: "December 21, 2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Importing data
```{r}
library(dplyr)
#data <- read.csv("C:/Users/Toby/Dropbox/DATA/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/Subset4R.csv",stringsAsFactor=FALSE)
data <- read.csv("D:/Dropbox/DATA/2011_Schwoerer_subsistence_fuel_survey/Subset4R.csv",stringsAsFactor=FALSE)
#eliminating missing data and equipment not used
data2 <- subset(data, !is.na(an_gal))
data2 <- subset(data2, used=="Used this equipment")

#eliminating Chainsaws and generators
data2 <- data2%>%
  filter(resname!="Chainsaw" &resname!="Generator")

#subsetting data into transportation modes
boat <- subset(data2, boat==1 & used=="Used this equipment")
atv <- subset(data2, atv==1& used=="Used this equipment")
car <- subset(data2, car==1& used=="Used this equipment")
sm <- subset(data2, sm==1& used=="Used this equipment")
```

#Models to use, linear mixed models
For high-level discussion see: https://www.theanalysisfactor.com/extensions-general-linear-model/
Response variable: an_gal, the annual amount of gasoline consumed in gallons

##What probability distribution best fits an_gal
See https://ase.tufts.edu/gsc/gradresources/guidetomixedmodelsinr/mixed%20model%20guide.html
```{r}
library(MASS)
library(car)
library(dplyr)

gal_hh <- data2 %>%
  group_by(ID)%>%
  summarise(an_gal = sum(an_gal),
            usenum = sum(usenum))%>%
  filter(an_gal!=0)
#normal distribution fit
qqp(gal_hh$an_gal, "norm")
qqp(gal_hh$an_gal, "lnorm")
#gamma distribution fit
gamma <- fitdistr(gal_hh$an_gal, "gamma")
qqp(gal_hh$an_gal, "gamma", shape = gamma$estimate[[1]], rate = gamma$estimate[[2]])

#exponential distribution fit
qqplot(x=qexp(ppoints(100)), y=gal_hh$an_gal, main="Exponential Q-Q Plot",
       xlab="Theoretical Quantiles", ylab= "Your Data Quantiles")
qqline(gal_hh$an_gal, distribution=qexp)
```

Result: 
The normal distribution is close enough so we don't need to do any Penalized Quasi-Likelihood estimation.  

Outliers:
There are four outliers consuming more than ca. 5000 gal/year. We eliminate these outliers below. 

##Creating dataset for regression 
For now, we leave out the modes and aggregate fuel consumption across modes resulting in a dataset where each row is data for each household. There are no mode-specific variables

an_gal_total ~ sum_edible_wei_lbs + sum_mean_dist_M 

levels: ID, commname, 

```{r}
#grouping data: each row is one household 
hh_data <- data2%>%
  group_by(ID)%>%
  mutate(an_gal_total=sum(an_gal),
         usenum_total=sum(usenum))%>%
  subset(!is.na(price), select=-c(an_gal, survey_id, UniqueID, resname, fuel, saw, sm, atv, car, boat, vehicles))%>%
  distinct()

#eliminating NAs for edible weight variable
hh_data2 <- subset(hh_data, !is.na(sum_edible_wei_lbs))
#eliminating records that are below 1gal in gasoline consumption. 
hh_data2 <- subset(hh_data2, an_gal_total>1)

#Creating a community ID
hh_data2$commID <- with(hh_data2, ifelse(commname=="Alatna",1,ifelse(commname=="Allakaket",2,ifelse(commname=="Anaktuvuk Pass",3,ifelse(commname=="Beaver",4,ifelse(commname=="Bettles",5,ifelse(commname=="Dot Lake",6,ifelse(commname=="Dry Creek",7,ifelse(commname=="Evansville",8,ifelse(commname=="Healy Lake",9,ifelse(commname=="Tok",10,11)))))))))))
hh_data2$commID <- as.factor(hh_data2$commID)
hh_data2$ID <- as.factor(hh_data2$ID)

#eliminating outliers consuming more than 5000 gallons
hh_data2 <- subset(hh_data2,an_gal_total <5000)
#checking normal fit of the data
#normal distribution fit
qqp(hh_data2$an_gal_total, "norm")
qqp(hh_data2$an_gal_total, "lnorm")

#calculating sampel size per community
samples <- hh_data2%>%
  group_by(commname, river)%>%
  summarise(n = n_distinct(ID))


#creating dataset without small sample communities
hh_data_s <- hh_data2%>%
  filter(commname!="Wiseman" & commname!="Beaver" & commname!="Dry Creek")

```


# Data exploration, looking for correlations
```{r}
library(PerformanceAnalytics)
my_data <- hh_data2[, c("an_gal_total","sum_edible_wei_lbs", "sum_mean_dist_M","usenum","FishSumEdible","LargeGameSumEdible")]
chart.Correlation(my_data, histogram=TRUE, pch=19)
```


my_car <- car[, c("an_gal","sum_edible_wei_lbs", "sum_mean_dist_M","usenum","FishSumEdible","LargeGameSumEdible")]
my_sm <-sm[, c("an_gal","sum_edible_wei_lbs", "sum_mean_dist_M","usenum","FishSumEdible","LargeGameSumEdible")]
my_atv <- atv[, c("an_gal","sum_edible_wei_lbs", "sum_mean_dist_M","usenum","FishSumEdible","LargeGameSumEdible")]
my_boat <- boat[, c("an_gal","sum_edible_wei_lbs", "sum_mean_dist_M","usenum","FishSumEdible","LargeGameSumEdible")]
pooled_boat <- boat[, c("an_gal","sum_edible_wei_lbs", "sum_mean_dist_M","usenum","FishSumEdible","LargeGameSumEdible","commname","ID")]

chart.Correlation(my_car, histogram=TRUE, pch=19)
chart.Correlation(my_sm, histogram=TRUE, pch=19)
chart.Correlation(my_atv, histogram=TRUE, pch=19)
chart.Correlation(my_boat, histogram=TRUE, pch=19)

Results:
an_gal_total ~ sum_edible_wei_lbs 0.21
an_gal_total ~ sum_mean_dist_M 0.038
car: an_gal ~ FishSumEdible 0.31
sm: an_gal ~ sum_edible_wei_lbs 0.2
atv: an_gal ~ sum_edible_wei_lbs 0.26
boat: an_gal ~ usenum 0.48, FishSumEdible 0.31, sum_edible_wei_lbs 0.27, LargeGameSumEdible 0.18

#Linear model
```{r}
linearMod <- lm(an_gal_total ~ sum_edible_wei_lbs , data=hh_data2)  # build linear regression model on full data
summary(linearMod)
```
Everything is highly significant in a simple model where the predictor of annual gasoline consumption per household is household's annual weight harvested.

The average subsistence household in 2011 consumed 551 gallons and for every lbs harveted an additional 0.1 gallons. 

Compare this to what each US household on average consumed in motor gasoline in 2017. 126.22 million households consumed 143 billion gallons according to EIA. https://www.eia.gov/energyexplained/index.php?page=gasoline_use
This translates to 1132 gallons per year and hh. 


#Fixed effects model
model indicates the model to be estimated : "pooling" is just the OLS estimation (equivalent to a call to lm), "between" performs the estimation on the individual or time means, "within" on the deviations from the individual or/and time mean, "fd" on the first differences and "random" perform a feasible generalized least squares estimation which takes into account the correlation induced by the presence of individual and/or time effects.
```{r}
library(plm)
fixed <- plm(an_gal ~ usenum + FishSumEdible , data=pooled_boat, index = "ID", model="random", effect = "individual", na.action = na.exclude)
summary(fixed)
```


# M1 Varying intercept model with no predictors (Variance components model)
Following https://mc-stan.org/users/documentation/case-studies/tutorial_rstanarm.html#model-3-varying-intercept-and-slope-model-with-a-single-predictor
```{r}
library(lme4)
M1 <- lmer(formula = an_gal_total ~ 1 + (1 | commID), 
           data = hh_data2, 
           REML = FALSE)
summary(M1)
```
Results: 
*Fixed effects*: 
These measure the gasoline consumption averaged across the population of communities is estimated at 1932 gallons per household. U

*Random effects*
Thees measure both the between community and within-community variation. In specific, the between-community SD is estimated at 524.7 gallons, and the SD within-communities is estimated to be 1899.6 gallons. 


# M2: Varying intercept model with a single predictor (sum of edible weight harvested)
Following https://mc-stan.org/users/documentation/case-studies/tutorial_rstanarm.html#model-3-varying-intercept-and-slope-model-with-a-single-predictor
```{r}
M2 <- lmer(formula = an_gal_total ~ 1 + sum_edible_wei_lbs + (1 | commname), 
           data = hh_data2, 
           REML = FALSE)
summary(M2)
commIntercepts <- ranef(M2)$commname
```


#M3: Varying intercept and slope model with a single predictor
```{r}
M3 <- lmer(formula = an_gal_total ~ 1 + sum_edible_wei_lbs + usenum_total + (1 + sum_edible_wei_lbs + usenum_total | commname), 
           data = hh_data2, 
           REML = FALSE)
summary(M3)
commInterceptsM3 <- ranef(M3)$commname
isSingular(M2, tol = 1e-05)
```

Best model to fit, AIC 2035, no fuel price, random intercept model
```{r}
Mtest <- lmer(formula = an_gal_total ~ 1 + FishSumEdible +LargeGameSumEdible + usenum_total   + comm_fuel_price + (1 | commname), 
           data = hh_data2, 
           REML = FALSE)
summary(Mtest)
isSingular(Mtest, tol = 1e-05)
commInterceptsMtest <- ranef(Mtest)$commname
```

Best intuitive model fit, AIC 2038, uses random intercept and slope model depending on fuel price
```{r}
Mtest2 <- lmer(formula = an_gal_total ~ 1 + FishSumEdible +LargeGameSumEdible + usenum_total   +  ( 1+ comm_fuel_price | commname), 
           data = hh_data2, 
           REML = FALSE)
summary(Mtest2)
commInterceptsMtest2 <- ranef(Mtest2)$commname
isSingular(Mtest2, tol = 1e-05)
```
But the above has a singularity. Let's try a Bayesian model


##Bayesian approach
https://mc-stan.org/users/documentation/case-studies/tutorial_rstanarm.html#model-3-varying-intercept-and-slope-model-with-a-single-predictor

```{r}
library(rstanarm)
Mtest2_stanlmer <- stan_lmer(formula = an_gal_total ~ 1 + FishSumEdible +LargeGameSumEdible + usenum_total   +  ( 1+ comm_fuel_price | commname), data = hh_data2, seed = 349)
prior_summary(object = M3_stanlmer)
Mtest2_stanlmer
summary(Mtest2_stanlmer)
```

above model not working out, weird results, so sticking to the initial Mtest which did not have a singularity and running it as a bayesian model below
```{r}
library(rstanarm)
M1_stanlmer <- stan_lmer(formula = an_gal_total ~ 1 + FishSumEdible +LargeGameSumEdible + usenum_total   + comm_fuel_price + (1 | commname), data = hh_data2, seed = 349)

prior_summary(object = M1_stanlmer)
M1_stanlmer
summary(M1_stanlmer)

#Evaluating model convergence and diagnose model

plot(M1_stanlmer, "rhat")
plot(M1_stanlmer, "ess")
launch_shinystan(M1_stanlmer)
```

above model shows high uncertainty in community specific intercepts, what happens if we leave those out. Also, model predicts the lowest consumption as negative. But since we only care about the median point estimates and the uncertainty in general this may not be an issue. 

```{r}
library(rstanarm)
M2 <- stan_glm(formula = an_gal_total ~  FishSumEdible +LargeGameSumEdible + usenum_total   + comm_fuel_price , data = hh_data2, seed = 349)

prior_summary(object = M2)
M2
summary(M2)

#Evaluating model convergence and diagnose model

plot(M2, "rhat")
plot(M2, "ess")
launch_shinystan(M2)
```

as expected, the intercept will take all the effects that the comm-specific intercepts specified previously, so not a good model. Above is better. 

















#Accessing the simulations and summarizing results
following https://mc-stan.org/users/documentation/case-studies/tutorial_rstanarm.html#model-3-varying-intercept-and-slope-model-with-a-single-predictor
```{r}
sims <- as.matrix(M1_stanlmer)
dim(sims)
para_name <- colnames(sims)
para_name

# Obtain school-level varying intercept a_j
# draws for overall mean
mu_a_sims <- as.matrix(M1_stanlmer, 
                       pars = "(Intercept)")
# draws for 73 schools' school-level error
u_sims <- as.matrix(M1_stanlmer, 
                    regex_pars = "b\\[\\(Intercept\\) commname\\:")
# draws for 73 schools' varying intercepts               
a_sims <- as.numeric(mu_a_sims) + u_sims          

# Obtain sigma_y and sigma_alpha^2
# draws for sigma_y
s_y_sims <- as.matrix(M1_stanlmer, 
                       pars = "sigma")
# draws for sigma_alpha^2
s__alpha_sims <- as.matrix(M1_stanlmer, 
                       pars = "Sigma[commname:(Intercept),(Intercept)]")
```

#Obtaining means, standard deviations, medians and 95% credible intervals.
```{r}
# Compute mean, SD, median, and 95% credible interval of varying intercepts

# Posterior mean and SD of each alpha
a_mean <- apply(X = a_sims,     # posterior mean
                MARGIN = 2,
                FUN = mean)
a_sd <- apply(X = a_sims,       # posterior SD
              MARGIN = 2,
              FUN = sd)

# Posterior median and 95% credible interval
a_quant <- apply(X = a_sims, 
                 MARGIN = 2, 
                 FUN = quantile, 
                 probs = c(0.025, 0.50, 0.975))
a_quant <- data.frame(t(a_quant))
names(a_quant) <- c("Q2.5", "Q50", "Q97.5")

# Combine summary statistics of posterior simulation draws
a_df <- data.frame(a_mean, a_sd, a_quant)
round(a_df, 2)
```

We can produce a caterpillar plot to show the fully Bayes estimates for the school varying intercepts in rank order together with their 95% credible intervals.
```{r}
# Sort dataframe containing an estimated alpha's mean and sd for every school
a_df <- a_df[order(a_df$a_mean), ]
a_df$a_rank <- c(1 : dim(a_df)[1])  # a vector of school rank 

# Plot school-level alphas's posterior mean and 95% credible interval
ggplot(data = a_df, 
       aes(x = a_rank, 
           y = a_mean)) +
  geom_pointrange(aes(ymin = Q2.5, 
                      ymax = Q97.5),
                  position = position_jitter(width = 0.1, 
                                             height = 0)) + 
  geom_hline(yintercept = mean(a_df$a_mean), 
             size = 0.5, 
             col = "red") + 
  scale_x_continuous("Rank", 
                     breaks = seq(from = 0, 
                                  to = 80, 
                                  by = 5)) + 
  scale_y_continuous(expression(paste("varying intercept, ", alpha[j]))) + 
  theme_bw( base_family = "serif")

```

# Making comparisons between individual communities, Aanktuvuk Pass and Tok
```{r}
comm_diff <- a_sims[, 3] - a_sims[, 7]
# Investigate differences of two distributions
mean <- mean(comm_diff)
sd <- sd(comm_diff)
quantile <- quantile(comm_diff, probs = c(0.025, 0.50, 0.975))
quantile <- data.frame(t(quantile))
names(quantile) <- c("Q2.5", "Q50", "Q97.5")
diff_df <- data.frame(mean, sd, quantile)
round(diff_df, 2)

#proportion of Anaktuvuk Pass households with higher mean consumption than Tok hhs

prop.table(table(a_sims[, 3] > a_sims[, 7]))
```


























#Stroring model results
Following https://www.drbanderson.com/2018/07/06/predicted-probabilities-with-rstanarm/
```{r}
library(broom)
model.results <- tidy(M1_stanlmer) %>% 
  mutate_if(is.numeric, funs(round(., 2)))
model.results
launch_shinystan(M1_stanlmer)
```





















```{r}
conditions <- c("Alatna","Allakaket","Anaktuvuk Pass","Beaver","Bettles","Dot Lake","Dry Creek","Evansville","Healy Lake","Tok","Wiseman")
```

Next comes defining a new dataframe. The posterior_linpred function draws on R’s predict function, which can take in a new dataframe as input. We want to predict the probability of happiness at each of the four possible experimental conditions. But because our initial model contains covariates, we need to include these as well.

This is where mean-centering the continuous predictors comes in handy, as does factoring the categorical variable, Gender…
```{r}
conditions.df <- data.frame(FishSumEdible = mean(hh_data2$FishSumEdible,na.rm = TRUE),
                            LargeGameSumEdible = mean(hh_data2$LargeGameSumEdible,na.rm = TRUE),
                            usenum_total = mean(hh_data2$usenum_total,na.rm = TRUE),
                            comm_fuel_price = mean(hh_data2$comm_fuel_price,na.rm = TRUE))
conditions.df
```


# Call the getProbabilities function to calculate the predicted probabilities.
```{r}
model.prob <- getProbabilities(M1_stanlmer)

# Call the createProbTable function to create the predicted probabilities table.
model.prob.table <- createProbTable(model.prob)
model.prob.table %>% 
  mutate_if(is.numeric, funs(100 * round(., 2))) # Convert to percentages
```







```{r}
library(stanreg)
library(dplyr)
#ABC is the raw data
ABC <- dplyr::select(hh_data2, commname, an_gal_total)
#converting to tibble
library(tibble)
ABC <- as_tibble(ABC)


pred_post <- posterior_predict(M1_stanlmer, newdata = ABC)
dim(pred_post)
```









#extracting draws from the fit in tidy-format, then visualizing them
https://cran.r-project.org/web/packages/tidybayes/vignettes/tidy-rstanarm.html

```{r}
library(magrittr)
library(dplyr)
library(forcats)
library(tidyr)
library(purrr)
library(modelr)
library(tidybayes)
library(ggplot2)
library(ggstance)
library(ggridges)
library(rstan)
library(rstanarm)
library(cowplot)
library(RColorBrewer)
library(gganimate)

get_variables(Mtest_stanlmer)

Mtest_stanlmer%>%
  spread_draws(`(Intercept)`, b[,group]) %>%
  mutate(condition_mean = `(Intercept)` + b) %>%
  median_qi(condition_mean)
```

Intervals with densities
To see the density along with the intervals, we can use geom_eyeh (horizontal “eye plots”, which combine intervals with violin plots), or geom_halfeyeh (horizontal interval + density plots):
```{R}
Mtest_stanlmer%>%
  spread_draws(`(Intercept)`, b[,group]) %>%
  mutate(condition_mean = `(Intercept)` + b) %>%
  ggplot(aes(y = group, x = condition_mean)) +
  geom_halfeyeh() 
```

Displaying posterior densities
```{r}
library(tidybayes)
Mtest_stanlmer%>%
  spread_draws(condition_mean[commname]) %>%
  ggplot(aes(y = fct_rev(commname), x = condition_mean)) +
  geom_violinh(color = NA, fill = "gray65") +
  stat_pointintervalh(.width = c(.95, .66))

```








Posterior predictions
Note, dataframes are not supported below
```{r}
library(dplyr)
ABC <- dplyr::select(hh_data2, commname, an_gal_total)
#converting to tibble
library(tibble)
ABC <- as_tibble(ABC)

ABC %>%
  ggplot(aes(y = commname, x = an_gal_total)) +
  geom_point()

ABC %>%
  data_grid(commname) %>%
  add_predicted_draws(hh_data2, Mtest_stanlmer)%>%
  ggplot(aes(y = commname, x = .prediction)) +
  stat_intervalh() +
  geom_point(aes(x = an_gal_total), data = ABC) +
  scale_color_brewer()

ABC %>%
  data_grid(commname) %>%
  add_predicted_draws(hh_data2, Mtest_stanlmer) %>%
  ggplot(aes(x = .prediction, y = commname)) +
  geom_density_ridges()

```

```{r}
grid <- ABC %>%
  data_grid(commname)

fits <- grid %>%
  add_fitted_draws(Mtest_stanlmer, re_formula = NA)

preds = grid %>%
  add_predicted_draws(Mtest_stanlmer,re_formula = NA )

ABC %>%
  ggplot(aes(y = commname, x = an_gal_total)) +
  stat_intervalh(aes(x = .prediction), data = preds) +
  stat_pointintervalh(aes(x = .value), data = fits, .width = c(.66, .95), position = position_nudge(y = -0.2)) +
  geom_point() +
  scale_color_brewer()


```




# Complete-pooling regression
```{r}
J <- length(unique(hh_data2$commname))
N <- nrow(hh_data2)

#complete pooling model, blue solid line in plot
pooled <- lm(formula = an_gal_total ~ sum_edible_wei_lbs + usenum_total,
             data = hh_data2)
a_pooled <- coef(pooled)[1]   # complete-pooling intercept
b_pooled <- coef(pooled)[2]   # complete-pooling slope

# No-pooling regression, red dashed line in plot
nopooled <- lm(formula = an_gal_total ~ 0 + commname + sum_edible_wei_lbs + usenum_total,
               data = hh_data2)
a_nopooled <- coef(nopooled)[1:J]   # 11 no-pooling intercepts              
b_nopooled <- coef(nopooled)[J+1]

# Partial pooling (multilevel) regression, purple dotted line in plot
a_part_pooled <- coef(M3)$commname[, 1]
b_part_pooled <- coef(M3)$commname[, 2]
```

Plotting community -specific regression lines for edible weights:
```{r}
# setting axis
y <- hh_data2$an_gal_total
x <- as.numeric(hh_data2$sum_edible_wei_lbs) - 1 + runif(N, -.05, .05)
schid <- hh_data2$commname

# generate data frame
df <- data.frame(y, x, schid)


# (2) Assign complete-pooling, no-pooling, partial pooling estimates
df$a_pooled <- a_pooled 
df$b_pooled <- b_pooled
df$a_nopooled <- a_nopooled[df$schid]
df$b_nopooled <- b_nopooled
df$a_part_pooled <- a_part_pooled[df$schid]
df$b_part_pooled <- b_part_pooled[df$schid]

# (3) Plot regression fits for the 11 communities
library(ggplot2)
ggplot(data = df, 
       aes(x = x, y = y)) + 
  facet_wrap(facets = ~ schid, 
             ncol = 4) + 
  theme_bw() +
  geom_jitter(position = position_jitter(width = .05, 
                                         height = 0)) +
  geom_abline(aes(intercept = a_pooled, 
                  slope = b_pooled), 
              linetype = "solid", 
              color = "blue", 
              size = 0.5) +
  geom_abline(aes(intercept = a_nopooled, 
                  slope = b_nopooled), 
              linetype = "longdash", 
              color = "red", 
              size = 0.5) + 
  geom_abline(aes(intercept = a_part_pooled, 
                  slope = b_part_pooled), 
              linetype = "dotted", 
              color = "purple", 
              size = 0.7) + 
 
                  
  labs(title = "Complete-pooling, No-pooling, and Partial pooling estimates",
       x = "Annual edible harvest in lbs", 
       y = "Annual consumption of gasoline in gal")+theme_bw( base_family = "serif")
```

Plotting community -specific regression lines for edible weights:
```{r}
# setting axis
y <- hh_data2$an_gal_total
x <- as.numeric(hh_data2$usenum_total) - 1 + runif(N, -.05, .05)
schid <- hh_data2$commname

# generate data frame
df <- data.frame(y, x, schid)


# (2) Assign complete-pooling, no-pooling, partial pooling estimates
df$a_pooled <- a_pooled 
df$b_pooled <- b_pooled
df$a_nopooled <- a_nopooled[df$schid]
df$b_nopooled <- b_nopooled
df$a_part_pooled <- a_part_pooled[df$schid]
df$b_part_pooled <- b_part_pooled[df$schid]

# (3) Plot regression fits for the 11 communities
library(ggplot2)
ggplot(data = df, 
       aes(x = x, y = y)) + 
  facet_wrap(facets = ~ schid, 
             ncol = 4) + 
  theme_bw() +
  geom_jitter(position = position_jitter(width = .05, 
                                         height = 0)) +
  geom_abline(aes(intercept = a_pooled, 
                  slope = b_pooled), 
              linetype = "solid", 
              color = "blue", 
              size = 0.5) +
  geom_abline(aes(intercept = a_nopooled, 
                  slope = b_nopooled), 
              linetype = "longdash", 
              color = "red", 
              size = 0.5) + 
  geom_abline(aes(intercept = a_part_pooled, 
                  slope = b_part_pooled), 
              linetype = "dotted", 
              color = "purple", 
              size = 0.7) + 
 
                  
  labs(title = "Complete-pooling, No-pooling, and Partial pooling estimates",
       x = "number of vehicles operated", 
       y = "Annual consumption of gasoline in gal")+theme_bw( base_family = "serif")
```























#standardizing both, an_gal_total and sum_edible_wei_lbs
calling them gal_stand and edi_stand respectively

```{r}
mean_gal <- mean(hh_data2$an_gal_total)
sd_gal <- sd(hh_data2$an_gal_total)
mean_lbs <- mean(hh_data2$sum_edible_wei_lbs)
sd_lbs <- sd(hh_data2$sum_edible_wei_lbs)

hh_data2$gal_stand <- with(hh_data2, (an_gal_total - mean_gal)/sd_gal)
hh_data2$edi_stand <- with(hh_data2, (sum_edible_wei_lbs - mean_lbs)/sd_lbs)
```

#M4: Varying intercept and slope model with a single predictor rescaled
```{r}
M4 <- lmer(formula = an_gal_total ~ 1 + edi_stand + (1 + edi_stand | commname), 
           data = hh_data2, 
           REML = FALSE)
summary(M4)
commInterceptsM4 <- ranef(M4)$commname
```

Estimating intercepts and slope in each community in three ways then graphing those













##PQL Fitting a GLMM model with multivariate normal random effects, using Penalized Quasi-Likelihood.
Test whether we can use penalized quasilikelihood (PQL) or not. PQL is a flexible technique that can deal with non-normal data, unbalanced design, and crossed random effects. However, it produces biased estimates if your response variable fits a discrete count distribution, like Poisson or binomial, and the mean is less than 5 - or if your response variable is binary.

Note, Make absolutely sure that any an_gal_total < 1 is eleliminated from the data, otherwise the following code will not run. 


```{r}
library(MASS)
PQL <- glmmPQL(an_gal_total ~ sum_edible_wei_lbs, random = ~ 1 | commID, family = Gamma(link="inverse"), data = hh_data2, verbose = FALSE)
summary(PQL)

```
the option random=~1|commname is added to the model to indicate that commname is the random term.
The 1 indicates that an intercept is to be fitted for each level of the random variable. Source: https://rcompanion.org/handbook/G_03.html

Turns out PQL is not as reliable method https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html














## Laplace approximation
Source: https://ase.tufts.edu/gsc/gradresources/guidetomixedmodelsinr/mixed%20model%20guide.html

```{r}
library(mlmRev)

GHQ <- glmer(an_gal_total ~ sum_edible_wei_lbs  + (1 | commname), data = hh_data2,
    family = Gamma(link="inverse"), nAGQ = 25)  # Set nAGQ to # of desired iterations
```


